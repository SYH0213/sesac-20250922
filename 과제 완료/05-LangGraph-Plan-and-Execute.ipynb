{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c95e9bf",
   "metadata": {},
   "source": [
    "# Plan-and-Execute\n",
    "\n",
    "이 튜토리얼에서는 \"plan-and-execute\" 스타일의 에이전트를 만드는 방법을 소개하고, 이를 [LangGraph](https://langchain-ai.github.io/langgraph/)를 활용하여 구현하는 과정을 단계별로 설명합니다.  \n",
    "\"plan-and-execute\" 전략은 복잡한 작업을 수행할 때 장기적인 계획을 먼저 수립한 뒤, 해당 계획을 단계별로 실행하며 필요에 따라 다시 계획을 재수정하는 접근법입니다.\n",
    "\n",
    "![](./assets/langgraph-plan-and-execute.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Plan-and-Execute란 무엇인가?\n",
    "\n",
    "\"plan-and-execute\"는 다음과 같은 특징을 갖는 접근 방식입니다.\n",
    "\n",
    "- **장기 계획 수립**: 복잡한 작업을 수행하기 전에 큰 그림을 그리는 장기 계획을 수립합니다.\n",
    "- **단계별 실행 및 재계획**: 세운 계획을 단계별로 실행하고, 각 단계가 완료될 때마다 계획이 여전히 유효한지 검토한 뒤 수정할 수 있습니다.\n",
    "  \n",
    "이 방식은 [Plan-and-Solve 논문](https://arxiv.org/abs/2305.04091)과 [Baby-AGI 프로젝트](https://github.com/yoheinakajima/babyagi)에서 영감을 받았습니다. 전통적인 [ReAct 스타일](https://arxiv.org/abs/2210.03629)의 에이전트는 한 번에 한 단계씩 생각하는 반면, \"plan-and-execute\"는 명시적이고 장기적인 계획을 강조합니다.\n",
    "\n",
    "**장점**:\n",
    "1. **명시적인 장기 계획**: 강력한 LLM조차도 한 번에 장기 계획을 처리하는 데 어려움을 겪을 수 있습니다. 명시적으로 장기 계획을 수립함으로써, 보다 안정적인 진행이 가능합니다.\n",
    "2. **효율적인 모델 사용**: 계획 단계에서는 더 큰/강력한 모델을 사용하고, 실행 단계에서는 상대적으로 작은/약한 모델을 사용함으로써 자원 소비를 최적화할 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "**주요 내용**\n",
    "\n",
    "- **도구 정의**: 사용할 도구 정의\n",
    "- **실행 에이전트 정의**: 실제 작업을 실행하는 에이전트 생성\n",
    "- **상태 정의**: 에이전트의 상태 정의\n",
    "- **계획 단계**: 장기 계획을 세우는 단계 생성\n",
    "- **재계획 단계**: 작업 진행 상황에 따라 계획을 재수정하는 단계 생성\n",
    "- **그래프 생성 및 실행**: 이러한 단계들을 연결하는 그래프 생성 및 실행\n",
    "\n",
    "---\n",
    "\n",
    "**참고**\n",
    "\n",
    "- [LangGraph 공식 문서](https://langchain-ai.github.io/langgraph/)  \n",
    "- [Plan-and-Solve 논문](https://arxiv.org/abs/2305.04091)  \n",
    "- [Baby-AGI 프로젝트](https://github.com/yoheinakajima/babyagi)  \n",
    "- [ReAct 논문](https://arxiv.org/abs/2210.03629)\n",
    "\n",
    "지금부터는 각 단계를 따라가며 \"plan-and-execute\" 에이전트를 LangGraph로 구현하는 방법을 자세히 알아보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99fefd6",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb1ad59b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b4bac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH17-LangGraph-Use-Cases\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH17-LangGraph-Use-Cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6afbb11",
   "metadata": {},
   "source": [
    "실습에 활용할 모델명 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afe4377f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemini-2.5-pro'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_teddynote.models import get_model_name, LLMs\n",
    "\n",
    "# 모델명 정의\n",
    "#MODEL_NAME = get_model_name(LLMs.GPT4o)\n",
    "MODEL_NAME = \"gemini-2.5-pro\"\n",
    "MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0856a092",
   "metadata": {},
   "source": [
    "## 도구 정의\n",
    "\n",
    "사용할 도구를 먼저 정의합니다. 이 간단한 예제에서는 `Tavily`를 통해 제공되는 내장 검색 도구를 사용할 것입니다. 그러나 직접 도구를 만드는 것도 매우 쉽습니다. \n",
    "\n",
    "자세한 내용은 [도구(Tools)](https://wikidocs.net/262582) 문서를 참조하십시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc76788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.tools import TavilySearch\n",
    "\n",
    "# Tavily 검색 도구 초기화\n",
    "tools = [TavilySearch(max_results=3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d230b5c",
   "metadata": {},
   "source": [
    "## 작업 실행 에이전트 정의\n",
    "\n",
    "이제 작업을 실행할 `execution agent`를 생성합니다. \n",
    "\n",
    "이 예제에서는 각 작업에 동일한 `execution agent`를 사용할 예정이지만, 반드시 이렇게 해야 하는 것은 아닙니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f70cf9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 프롬프트 정의\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer in Korean.\",\n",
    "        ),\n",
    "        (\"human\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# LLM 정의\n",
    "llm = ChatGoogleGenerativeAI(temperature=0, model=MODEL_NAME)\n",
    "\n",
    "# ReAct 에이전트 생성\n",
    "agent_executor = create_react_agent(llm, tools, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab1307f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='랭체인 한국어 튜토리얼에 대해서 설명해줘', additional_kwargs={}, response_metadata={}, id='2f885b56-f57e-4e70-9dfd-579ecc71a2c2'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'tavily_web_search', 'arguments': '{\"query\": \"\\\\ub7ad\\\\uccb4\\\\uc778 \\\\ud55c\\\\uad6d\\\\uc5b4 \\\\ud29c\\\\ud1a0\\\\ub9ac\\\\uc5bc\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-pro', 'safety_ratings': []}, id='run--b5152209-ac40-408f-bae4-fce01669d259-0', tool_calls=[{'name': 'tavily_web_search', 'args': {'query': '랭체인 한국어 튜토리얼'}, 'id': '1327adf2-6963-4dc4-a9bc-09bec7f2179d', 'type': 'tool_call'}]),\n",
       "  ToolMessage(content='[{\"url\": \"https://m.blog.naver.com/htk1019/223388363309\", \"title\": \"LangChain 공부하기 진짜진짜 좋은 자료 - 네이버 블로그\", \"content\": \"*My Menu 닫기* 본문 바로가기 * **본문 폰트 크기 조정** **<랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷** \\\\\\\\*\\\\\\\\*추천\\\\\\\\*\\\\\\\\*은 공유할 수 있는 무료 전자책을 집필하는데 정말 큰 힘이 됩니다. 만들어 주신분께 감사드립니다. ### 이 블로그**ML** 카테고리 글 * 현재 하고 있는 일들.. * 투자 이론 교육과 비법은 다릅니다. 투자 이론 교육이 낚시하는 법을 알려주는 것이라면 투자 비법은 낚시 하는 족족 대어를 낚는 방법을 알려주는 것입니다. 그러나 비법을 찾는 것은 다릅니다. 거기에 \\'쉽다\\' 라는 워딩까지 첨가되었다면 사기에 가깝다고 보면 됩니다. 문서를 작은 조각으로 나누는 이유는 LLM 모델이 긴 문서를 잘 처리하지 못해서이다.... 개발자들이 챗봇, 질의응답 시스템, 자동 요약 등 다양한 LLM 애플리케이션을 쉽게 개발할 수 있도록 지원하는 도구모음(프레임워크) LangChain 도구의 구성 1. 이미 부품들을 조합한 예시들을 만들어 놨기 때문에 사용자는 잘 쓰면... {\\\\\"title\\\\\":\\\\\"LangChain 공부하기 진짜진짜 좋은 자료\\\\\",\\\\\"source\\\\\":\\\\\"https://blog.naver.com/htk1019/223388363309\\\\\",\\\\\"blogName\\\\\":\\\\\"엉드루 블..\\\\\",\\\\\"domainIdOrBlogId\\\\\":\\\\\"htk1019\\\\\",\\\\\"nicknameOrBlogId\\\\\":\\\\\"엉드루\\\\\",\\\\\"logNo\\\\\":223388363309,\\\\\"smartEditorVersion\\\\\":4,\\\\\"lineDisplay\\\\\":true,\\\\\"outsideDisplay\\\\\":false,\\\\\"meDisplay\\\\\":true,\\\\\"cafeDisplay\\\\\":true,\\\\\"blogDisplay\\\\\":true} 닫기\", \"score\": 0.80698997, \"raw_content\": \"[로그인이 필요합니다.](#)\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n*My Menu 닫기*\\\\n\\\\n[본문 바로가기](#ct)\\\\n\\\\n[검색](/PostSearchList.naver?blogId=htk1019)\\\\n\\\\n[ML](/PostList.naver?blogId=htk1019&categoryNo=25&logCode=0&categoryName=ML#postlist_block)\\\\n\\\\nLangChain 공부하기 진짜진짜 좋은 자료\\\\n\\\\n[**엉드루**](/PostList.naver?blogId=htk1019)\\\\n\\\\n2024. 3. 19. 15:37\\\\n\\\\n[이웃추가](#)\\\\n\\\\n\\\\n* **본문 폰트 크기 조정**\\\\n  가\\\\n* [*공감*](#)\\\\n* [공유하기](#)\\\\n* [URL복사](#)\\\\n* [신고하기](#)\\\\n\\\\n\\\\n\\\\n\\\\n<https://wikidocs.net/book/14314>\\\\n\\\\n[**<랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷**\\\\n\\\\n\\\\\\\\*\\\\\\\\*추천\\\\\\\\*\\\\\\\\*은 공유할 수 있는 무료 전자책을 집필하는데 정말 큰 힘이 됩니다. \\\\\\\\*\\\\\\\\*추천\\\\\\\\*\\\\\\\\* 한 번씩만 부탁 드리겠습니다🙏🙏 ✅ \\\\\\\\*\\\\\\\\*랭체인 한국어 튜토리얼 코드저장소(Gi…\\\\n\\\\nwikidocs.net](https://wikidocs.net/book/14314)\\\\n\\\\n만들어 주신분께 감사드립니다.\\\\n\\\\n\\u200b\\\\n\\\\n실적 시즌이라 개인 포트폴리오 열심히 보는 중이지만 마스터 해야겠습니다.\\\\n\\\\n\\u200b\\\\n\\\\n\\u200b\\\\n\\\\n[*12*](#)\\\\n\\\\n[공감한 사람 보러가기](/SympathyHistoryList.naver?blogId=htk1019&logNo=223388363309&categoryId=POST)\\\\n\\\\n[댓글 *0*](/CommentList.naver?blogId=htk1019&logNo=223388363309)\\\\n공유하기\\\\n\\\\n[**엉드루**\\\\n\\\\n일상·생각이웃7,897명](/htk1019)\\\\n\\\\n[### 이 블로그**ML** 카테고리 글](https://m.blog.naver.com/PostList.naver?blogId=htk1019&categoryNo=25&logCode=0&tab=1#postlist_block)\\\\n\\\\n* [LLM을 쓰려면 역시 LangChain 2편\\\\n\\\\n  1년 전공감17댓글3](https://m.blog.naver.com/htk1019/223413412145?recommendTrackingCode=2)\\\\n* [LLM을 쓰려면 역시 LangChain 1편\\\\n\\\\n  1년 전공감24댓글1](https://m.blog.naver.com/htk1019/223413194642?recommendTrackingCode=2)\\\\n* LangChain 공부하기 진짜진짜 좋은 자료\\\\n\\\\n  1년 전공감12댓글0\\\\n* [데이터 없이 만드는 주식분석 GPTS\\\\n\\\\n  1년 전공감30댓글5](https://m.blog.naver.com/htk1019/223313391721?recommendTrackingCode=2)\\\\n* [NeuroIPS 2023 최우수 논문상 수상 논문 4편 요약\\\\n\\\\n  1년 전공감17댓글1](https://m.blog.naver.com/htk1019/223310002666?recommendTrackingCode=2)\\\\n\\\\n[### 이 블로그 인기글 ✨](https://m.blog.naver.com/htk1019?anchorBlockType=popularpostlist)\\\\n\\\\n* [현재 하고 있는 일들..\\\\n\\\\n  1주 전공감50댓글8](https://m.blog.naver.com/htk1019/223979149493?recommendCode=2&recommendTrackingCode=2)\\\\n* [투자 이론 교육과 비법은 다릅니다. 투자 이론 교육이 낚시하는 법을 알려주는 것이라면 투자 비법은 낚시 하는 족족 대어를 낚는 방법을 알려주는 것입니다. 투자를 배우고 책을 읽는 행위는 올바른 투자자의 방식입니다. 그러나 비법을 찾는 것은 다릅니다. 투자에 \\'비법\\' 이라는 단어를 결합한 것이 있다면 그 어떤 강의나 책이든 모두 무가치 합니다. 거기에 \\'쉽다\\' 라는 워딩까지 첨가되었다면 사기에 가깝다고 보면 됩니다. ...\\\\n\\\\n  잡담...\\\\n\\\\n  3주 전공감57댓글6](https://m.blog.naver.com/htk1019/223957194867?recommendCode=2&recommendTrackingCode=2)\\\\n* [팔란티어 Foundary 관련 내용\\\\n\\\\n  2개월 전공감42댓글2](https://m.blog.naver.com/htk1019/223890433039?recommendCode=2&recommendTrackingCode=2)\\\\n* [지난 시간에는 문서들을 모듈을 통해 가져오는 과정을 진행했다. 이번에는 불러온 긴 문서를 작은 단위인 청크로 나누는 도구 청크로 나눠진 텍스트 데이터를 숫자로 변환(벡터변환) 이 두가지를 알아본다. 텍스트 분리하기 (Text Splitter) 텍스트를 분리하는 작업을 청킹(chunking) 이라고 한다. 나눠진 작은 조각 단위는 청크라고 한다. 문서를 작은 조각으로 나누는 이유는 LLM 모델이 긴 문서를 잘 처리하지 못해서이다....\\\\n\\\\n  LLM을 쓰려면 역시 LangChain 6편 - 임베딩\\\\n\\\\n  1년 전공감14댓글0](https://m.blog.naver.com/htk1019/223445162918?recommendCode=2&recommendTrackingCode=2)\\\\n* [LLM을 쓰려면 역시 LangChain 5편 - RAG\\\\n\\\\n  1년 전공감15댓글0](https://m.blog.naver.com/htk1019/223442628204?recommendCode=2&recommendTrackingCode=2)\\\\n* [궁금하지 않은 내용은 쏙 빼고 핵심만 정리한 LangChain 포스트 LangChain 이란? 개발자들이 챗봇, 질의응답 시스템, 자동 요약 등 다양한 LLM 애플리케이션을 쉽게 개발할 수 있도록 지원하는 도구모음(프레임워크) LangChain 도구의 구성 1. 라이브러리 : 파이썬과 자바스크립트 라이브러리 있음. 2. 템플릿 : 작업을 도와주는 \\'조립식 건축세트\\' . 이미 부품들을 조합한 예시들을 만들어 놨기 때문에 사용자는 잘 쓰면...\\\\n\\\\n  LLM을 쓰려면 역시 LangChain 1편\\\\n\\\\n  1년 전공감24댓글1](https://m.blog.naver.com/htk1019/223413194642?recommendCode=2&recommendTrackingCode=2)\\\\n\\\\n{\\\\\"title\\\\\":\\\\\"LangChain 공부하기 진짜진짜 좋은 자료\\\\\",\\\\\"source\\\\\":\\\\\"https://blog.naver.com/htk1019/223388363309\\\\\",\\\\\"blogName\\\\\":\\\\\"엉드루 블..\\\\\",\\\\\"domainIdOrBlogId\\\\\":\\\\\"htk1019\\\\\",\\\\\"nicknameOrBlogId\\\\\":\\\\\"엉드루\\\\\",\\\\\"logNo\\\\\":223388363309,\\\\\"smartEditorVersion\\\\\":4,\\\\\"lineDisplay\\\\\":true,\\\\\"outsideDisplay\\\\\":false,\\\\\"meDisplay\\\\\":true,\\\\\"cafeDisplay\\\\\":true,\\\\\"blogDisplay\\\\\":true}\\\\n\\\\n[닫기](#)\\\\n\\\\n[이 블로그 홈](/htk1019)\\\\n\\\\n[*12*](#)\\\\n\\\\n[공감한 사람 보러가기](/SympathyHistoryList.naver?blogId=htk1019&logNo=223388363309&categoryId=POST)\\\\n\\\\n[댓글 *0*](/CommentList.naver?blogId=htk1019&logNo=223388363309)\\\\n[공유하기](#)\\\\n\\\\n**엉드루(htk1019)**\\\\n님을 이웃추가하고 새글을 받아보세요\\\\n\\\\n[취소](#)\\\\n[이웃추가](#)\\\\n\\\\n[**0** / 0](javascript:void(0);)\"}, {\"url\": \"https://github.com/teddylee777/langchain-kr\", \"title\": \"teddylee777/langchain-kr - GitHub\", \"content\": \"본 튜토리얼을 통해 LangChain을 더 쉽고 효과적으로 사용하는 방법을 배울 수 있습니다. 본 튜토리얼을 통해 LangChain을 더 쉽고 효과적으로 사용하는 방법을 배울 수 있습니다. * 🔥성능이 놀라워요🔥 무료로 한국어🇰🇷 파인튜닝 모델 받아서 나만의 로컬 LLM 호스팅 하기(#LangServe) + #RAG 까지!! * 무료로 한국어🇰🇷 파인튜닝 모델 받아서 나만의 로컬 LLM 호스팅 하기(LangServe) + RAG 까지!! * OpenAI API 모델 리스트 / 요금표 ### OpenAI Python API ### LangChain * OpenAI GPT 모델(ChatOpenAI) 사용법 본 프로젝트는 Apache License 2.0에 따라 라이선스가 부여됩니다. 본 문서는 다른 문서의 내용을 참고하여 작성되었을 수 있습니다. 참고 자료는 본 문서 하단의 출처 목록에서 확인하실 수 있습니다. 본 튜토리얼을 통해 LangChain을 더 쉽고 효과적으로 사용하는 방법을 배울 수 있습니다. tutorial   cookbook   openai   huggingface   gpt-3   openai-api   gpt-4   generative-ai   chatgpt   langchain   chatgpt-api   langchain-python\", \"score\": 0.7777226, \"raw_content\": \"[Skip to content](#start-of-content)    \\\\n\\\\n\\\\n## Navigation Menu\\\\n\\\\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fteddylee777%2Flangchain-kr) \\\\n\\\\nAppearance settings\\\\n\\\\n# Search code, repositories, users, issues, pull requests...\\\\n\\\\n[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)\\\\n\\\\n[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2Fteddylee777%2Flangchain-kr)\\\\n\\\\n [Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=teddylee777%2Flangchain-kr) \\\\n\\\\nAppearance settings\\\\n\\\\nYou signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert\\\\n\\\\n{{ message }}\\\\n\\\\n[teddylee777](/teddylee777)   /  **[langchain-kr](/teddylee777/langchain-kr)**  Public\\\\n\\\\n* [Notifications](/login?return_to=%2Fteddylee777%2Flangchain-kr)  You must be signed in to change notification settings\\\\n* [Fork 648](/login?return_to=%2Fteddylee777%2Flangchain-kr)\\\\n* [Star  1.8k](/login?return_to=%2Fteddylee777%2Flangchain-kr)\\\\n\\\\nLangChain 공식 Document, Cookbook, 그 밖의 실용 예제를 바탕으로 작성한 한국어 튜토리얼입니다. 본 튜토리얼을 통해 LangChain을 더 쉽고 효과적으로 사용하는 방법을 배울 수 있습니다.\\\\n\\\\n[wikidocs.net/book/14314](https://wikidocs.net/book/14314 \\\\\"https://wikidocs.net/book/14314\\\\\")\\\\n\\\\n### License\\\\n\\\\n[Apache-2.0 license](/teddylee777/langchain-kr/blob/main/LICENSE)\\\\n\\\\n[1.8k stars](/teddylee777/langchain-kr/stargazers)   [648 forks](/teddylee777/langchain-kr/forks)   [Branches](/teddylee777/langchain-kr/branches)   [Tags](/teddylee777/langchain-kr/tags)   [Activity](/teddylee777/langchain-kr/activity)\\\\n\\\\n[Star](/login?return_to=%2Fteddylee777%2Flangchain-kr)\\\\n\\\\n[Notifications](/login?return_to=%2Fteddylee777%2Flangchain-kr)  You must be signed in to change notification settings\\\\n\\\\n# teddylee777/langchain-kr\\\\n\\\\n[Branches](/teddylee777/langchain-kr/branches)[Tags](/teddylee777/langchain-kr/tags)\\\\n\\\\nOpen more actions menu\\\\n\\\\n## Folders and files\\\\n\\\\n| Name | Name | Last commit message | Last commit date |\\\\n| --- | --- | --- | --- |\\\\n| Latest commit   History[407 Commits](/teddylee777/langchain-kr/commits/main/) |\\\\n| [01-Basic](/teddylee777/langchain-kr/tree/main/01-Basic \\\\\"01-Basic\\\\\") | [01-Basic](/teddylee777/langchain-kr/tree/main/01-Basic \\\\\"01-Basic\\\\\") |  |  |\\\\n| [02-Prompt](/teddylee777/langchain-kr/tree/main/02-Prompt \\\\\"02-Prompt\\\\\") | [02-Prompt](/teddylee777/langchain-kr/tree/main/02-Prompt \\\\\"02-Prompt\\\\\") |  |  |\\\\n| [03-OutputParser](/teddylee777/langchain-kr/tree/main/03-OutputParser \\\\\"03-OutputParser\\\\\") | [03-OutputParser](/teddylee777/langchain-kr/tree/main/03-OutputParser \\\\\"03-OutputParser\\\\\") |  |  |\\\\n| [04-Model](/teddylee777/langchain-kr/tree/main/04-Model \\\\\"04-Model\\\\\") | [04-Model](/teddylee777/langchain-kr/tree/main/04-Model \\\\\"04-Model\\\\\") |  |  |\\\\n| [05-Memory](/teddylee777/langchain-kr/tree/main/05-Memory \\\\\"05-Memory\\\\\") | [05-Memory](/teddylee777/langchain-kr/tree/main/05-Memory \\\\\"05-Memory\\\\\") |  |  |\\\\n| [06-DocumentLoader](/teddylee777/langchain-kr/tree/main/06-DocumentLoader \\\\\"06-DocumentLoader\\\\\") | [06-DocumentLoader](/teddylee777/langchain-kr/tree/main/06-DocumentLoader \\\\\"06-DocumentLoader\\\\\") |  |  |\\\\n| [07-TextSplitter](/teddylee777/langchain-kr/tree/main/07-TextSplitter \\\\\"07-TextSplitter\\\\\") | [07-TextSplitter](/teddylee777/langchain-kr/tree/main/07-TextSplitter \\\\\"07-TextSplitter\\\\\") |  |  |\\\\n| [08-Embeddings](/teddylee777/langchain-kr/tree/main/08-Embeddings \\\\\"08-Embeddings\\\\\") | [08-Embeddings](/teddylee777/langchain-kr/tree/main/08-Embeddings \\\\\"08-Embeddings\\\\\") |  |  |\\\\n| [09-VectorStore](/teddylee777/langchain-kr/tree/main/09-VectorStore \\\\\"09-VectorStore\\\\\") | [09-VectorStore](/teddylee777/langchain-kr/tree/main/09-VectorStore \\\\\"09-VectorStore\\\\\") |  |  |\\\\n| [10-Retriever](/teddylee777/langchain-kr/tree/main/10-Retriever \\\\\"10-Retriever\\\\\") | [10-Retriever](/teddylee777/langchain-kr/tree/main/10-Retriever \\\\\"10-Retriever\\\\\") |  |  |\\\\n| [11-Reranker](/teddylee777/langchain-kr/tree/main/11-Reranker \\\\\"11-Reranker\\\\\") | [11-Reranker](/teddylee777/langchain-kr/tree/main/11-Reranker \\\\\"11-Reranker\\\\\") |  |  |\\\\n| [12-RAG](/teddylee777/langchain-kr/tree/main/12-RAG \\\\\"12-RAG\\\\\") | [12-RAG](/teddylee777/langchain-kr/tree/main/12-RAG \\\\\"12-RAG\\\\\") |  |  |\\\\n| [13-LangChain-Expression-Language](/teddylee777/langchain-kr/tree/main/13-LangChain-Expression-Language \\\\\"13-LangChain-Expression-Language\\\\\") | [13-LangChain-Expression-Language](/teddylee777/langchain-kr/tree/main/13-LangChain-Expression-Language \\\\\"13-LangChain-Expression-Language\\\\\") |  |  |\\\\n| [14-Chains](/teddylee777/langchain-kr/tree/main/14-Chains \\\\\"14-Chains\\\\\") | [14-Chains](/teddylee777/langchain-kr/tree/main/14-Chains \\\\\"14-Chains\\\\\") |  |  |\\\\n| [15-Agent](/teddylee777/langchain-kr/tree/main/15-Agent \\\\\"15-Agent\\\\\") | [15-Agent](/teddylee777/langchain-kr/tree/main/15-Agent \\\\\"15-Agent\\\\\") |  |  |\\\\n| [16-Evaluations](/teddylee777/langchain-kr/tree/main/16-Evaluations \\\\\"16-Evaluations\\\\\") | [16-Evaluations](/teddylee777/langchain-kr/tree/main/16-Evaluations \\\\\"16-Evaluations\\\\\") |  |  |\\\\n| [17-LangGraph](/teddylee777/langchain-kr/tree/main/17-LangGraph \\\\\"17-LangGraph\\\\\") | [17-LangGraph](/teddylee777/langchain-kr/tree/main/17-LangGraph \\\\\"17-LangGraph\\\\\") |  |  |\\\\n| [18-FineTuning](/teddylee777/langchain-kr/tree/main/18-FineTuning \\\\\"18-FineTuning\\\\\") | [18-FineTuning](/teddylee777/langchain-kr/tree/main/18-FineTuning \\\\\"18-FineTuning\\\\\") |  |  |\\\\n| [19-Streamlit](/teddylee777/langchain-kr/tree/main/19-Streamlit \\\\\"19-Streamlit\\\\\") | [19-Streamlit](/teddylee777/langchain-kr/tree/main/19-Streamlit \\\\\"19-Streamlit\\\\\") |  |  |\\\\n| [20-Projects/01-ParsingOutput](/teddylee777/langchain-kr/tree/main/20-Projects/01-ParsingOutput \\\\\"This path skips through empty directories\\\\\") | [20-Projects/01-ParsingOutput](/teddylee777/langchain-kr/tree/main/20-Projects/01-ParsingOutput \\\\\"This path skips through empty directories\\\\\") |  |  |\\\\n| [22-OpenAI](/teddylee777/langchain-kr/tree/main/22-OpenAI \\\\\"22-OpenAI\\\\\") | [22-OpenAI](/teddylee777/langchain-kr/tree/main/22-OpenAI \\\\\"22-OpenAI\\\\\") |  |  |\\\\n| [99-Projects](/teddylee777/langchain-kr/tree/main/99-Projects \\\\\"99-Projects\\\\\") | [99-Projects](/teddylee777/langchain-kr/tree/main/99-Projects \\\\\"99-Projects\\\\\") |  |  |\\\\n| [images](/teddylee777/langchain-kr/tree/main/images \\\\\"images\\\\\") | [images](/teddylee777/langchain-kr/tree/main/images \\\\\"images\\\\\") |  |  |\\\\n| [.env\\\\\\\\_sample](/teddylee777/langchain-kr/blob/main/.env_sample \\\\\".env_sample\\\\\") | [.env\\\\\\\\_sample](/teddylee777/langchain-kr/blob/main/.env_sample \\\\\".env_sample\\\\\") |  |  |\\\\n| [.gitignore](/teddylee777/langchain-kr/blob/main/.gitignore \\\\\".gitignore\\\\\") | [.gitignore](/teddylee777/langchain-kr/blob/main/.gitignore \\\\\".gitignore\\\\\") |  |  |\\\\n| [.python-version](/teddylee777/langchain-kr/blob/main/.python-version \\\\\".python-version\\\\\") | [.python-version](/teddylee777/langchain-kr/blob/main/.python-version \\\\\".python-version\\\\\") |  |  |\\\\n| [LICENSE](/teddylee777/langchain-kr/blob/main/LICENSE \\\\\"LICENSE\\\\\") | [LICENSE](/teddylee777/langchain-kr/blob/main/LICENSE \\\\\"LICENSE\\\\\") |  |  |\\\\n| [README.md](/teddylee777/langchain-kr/blob/main/README.md \\\\\"README.md\\\\\") | [README.md](/teddylee777/langchain-kr/blob/main/README.md \\\\\"README.md\\\\\") |  |  |\\\\n| [poetry.lock](/teddylee777/langchain-kr/blob/main/poetry.lock \\\\\"poetry.lock\\\\\") | [poetry.lock](/teddylee777/langchain-kr/blob/main/poetry.lock \\\\\"poetry.lock\\\\\") |  |  |\\\\n| [pyproject.toml](/teddylee777/langchain-kr/blob/main/pyproject.toml \\\\\"pyproject.toml\\\\\") | [pyproject.toml](/teddylee777/langchain-kr/blob/main/pyproject.toml \\\\\"pyproject.toml\\\\\") |  |  |\\\\n| [requirements-mini.txt](/teddylee777/langchain-kr/blob/main/requirements-mini.txt \\\\\"requirements-mini.txt\\\\\") | [requirements-mini.txt](/teddylee777/langchain-kr/blob/main/requirements-mini.txt \\\\\"requirements-mini.txt\\\\\") |  |  |\\\\n| [requirements-onnx.txt](/teddylee777/langchain-kr/blob/main/requirements-onnx.txt \\\\\"requirements-onnx.txt\\\\\") | [requirements-onnx.txt](/teddylee777/langchain-kr/blob/main/requirements-onnx.txt \\\\\"requirements-onnx.txt\\\\\") |  |  |\\\\n| [requirements.txt](/teddylee777/langchain-kr/blob/main/requirements.txt \\\\\"requirements.txt\\\\\") | [requirements.txt](/teddylee777/langchain-kr/blob/main/requirements.txt \\\\\"requirements.txt\\\\\") |  |  |\\\\n|  |\\\\n\\\\n## Repository files navigation\\\\n\\\\n# 📘 LangChain 한국어 튜토리얼\\\\n\\\\n🌟 **LangChain 공식 Document, Cookbook, 그 밖의 실용 예제**를 바탕으로 작성한 한국어 튜토리얼입니다.\\\\n\\\\n본 튜토리얼을 통해 LangChain을 더 쉽고 효과적으로 사용하는 방법을 배울 수 있습니다.\\\\n\\\\n## 📔 위키독스 전자책(무료)\\\\n\\\\n위키독스에 **무료** 전자책을 등록하였습니다✌️\\\\n\\\\n위키독스 페이지에서 **책 \\\\\"추천\\\\\"** 버튼 한 번씩만 눌러 주시면 제작에 큰 힘이 됩니다. 미리 감사 드립니다🫶\\\\n\\\\n틈나는대로 열심히 업데이트 하고 있습니다. 앞으로도 신규 기능이 추가 될 때마다 빠르게 x100 **업데이트** 예정입니다.\\\\n\\\\n* [랭체인LangChain 노트 by 테디노트](https://wikidocs.net/book/14314) 구경하러 가기\\\\n\\\\n## 🍿 유튜브\\\\n\\\\n* [🤗 huggingface 에 공개된 오픈모델을 💻 로컬PC 에서 빠르게 실행🔥 해보고 테스트 하는 방법 + 모델 서빙🚀 + 업무자동화🤖 에 적용하는 방법까지!](https://youtu.be/bANQk--Maxs)\\\\n* [👀 코드 기반 답변하는 💻 GitHub 소스코드 기반 Q&A 챗봇🤖 제작기](https://youtu.be/yB8b-lW_5HY)\\\\n* [llama3 출시🔥 로컬에서 Llama3-8B 모델 돌려보기👀](https://youtu.be/12CuUQIPdM4)\\\\n* [🔥성능이 놀라워요🔥 무료로 한국어🇰🇷 파인튜닝 모델 받아서 나만의 로컬 LLM 호스팅 하기(#LangServe) + #RAG 까지!!](https://youtu.be/VkcaigvTrug)\\\\n* [무료로 한국어🇰🇷 파인튜닝 모델 받아서 나만의 로컬 LLM 호스팅 하기(LangServe) + RAG 까지!!](https://youtu.be/VkcaigvTrug)\\\\n* [Streamlit 으로 ChatGPT 클론 서비스 제작하는 방법](https://youtu.be/ZVmLe3odQvc)\\\\n* [대화내용을 기록하는 LLM Chain 생성 방법 + 도큐먼트 참조하는 tip!](https://youtu.be/VtS8yF2ItgI)\\\\n* [(Self Learning GPT) LangSmith 피드백으로 원하는 형식의 답변을 학습하는 GPT](https://youtu.be/8JEbrboSumg)\\\\n* [(LangServe 리뷰) 초간편 LLM 웹앱 제작 & 배포기능까지! 과연, Streamlit 대체할 수 있을까?](https://youtu.be/mdzMBF56HOM)\\\\n* [AI vs AI 의대 증원에 대한 모의 찬반토론 (AI 더빙본)](https://youtu.be/Z2j93efq1S0)\\\\n* [토론 AI 에이전트 - 의대 입학정원 증원에 대한 찬반토론을 AI 끼리 한다면?](https://youtu.be/NaU89YXQAoI)\\\\n* [긴 문서(long context) 에 대한 참신한 RAG 방법론: RAPTOR! 논문 리뷰와 코드를 준비했습니다](https://youtu.be/gcdkISrpMCA)\\\\n* [LangChain 밋업 발표 / R.A.G. 우리가 절대 쉽게결과물을 얻을 수 없는 이유](https://youtu.be/NfQrRQmDrcc)\\\\n* [노코딩으로 쇼핑몰 리뷰 분석 (크롤링 + Q&A 챗봇)](https://youtu.be/r5YyftofuII)\\\\n* [ChatGPT 의 GPTS 에 API 호출기능을 붙이면 어떻게 될까?](https://youtu.be/JBL4vhwUAnc)\\\\n* [LangChain Agent 를 활용하여 ChatGPT를 업무자동화 에 적용하는 방법🔥🔥](https://youtu.be/2q1XlbFVzf8)\\\\n* [Private GPT! 나만의 ChatGPT 만들기 (HuggingFace Open LLM 활용)](https://youtu.be/aECwQqBWfJM)\\\\n* [LangGraph 의 멀티 에이전트 콜라보레이션 찍먹하기](https://youtu.be/G8jrAA2bPnA)\\\\n* [마법같은 문법 LangChain Expression Language(LCEL)](https://youtu.be/0X4Ks_nJUt8)\\\\n* [이미지를 matplotlib 파이썬 코드로, 원하는 문장을 입력하면 파이썬 코드로 변환하는 방법](https://youtu.be/hY8Pquy3Hhg)\\\\n* [RAG 파이프라인 이해해보기 - 네이버 뉴스기사 기반 Q&A 챗봇 제작](https://youtu.be/1scMJH93v0M)\\\\n* [OpenAI 의 새로운 기능 Assistant API 완벽히 이해해보기](https://youtu.be/-Wne4a-8RlY)\\\\n* [OpenAI 의 새로운 기능 Assistant API 3가지 도구 활용법](https://youtu.be/BMW1NJkL7Ks)\\\\n\\\\n## ✏️ 블로그 글 목록\\\\n\\\\n### General\\\\n\\\\n* [OpenAI API 모델 리스트 / 요금표](https://teddylee777.github.io/openai/openai-models/)\\\\n\\\\n### OpenAI Python API\\\\n\\\\n* [OpenAI Python API 키 발급방법, 요금체계](https://teddylee777.github.io/openai/openai-api-key/)\\\\n* [채팅(chat) 함수 사용하기(1)](https://teddylee777.github.io/openai/openai-api-tutorial-01/)\\\\n* [DALL·E를 사용하여 이미지 생성, 수정, 다양화하기(2)](https://teddylee777.github.io/openai/openai-api-tutorial-02/)\\\\n* [Whisper API를 사용하여 TTS, STT 구현하기(3)](https://teddylee777.github.io/openai/openai-api-tutorial-03/)\\\\n\\\\n### LangChain\\\\n\\\\n* [OpenAI GPT 모델(ChatOpenAI) 사용법](https://teddylee777.github.io/langchain/langchain-tutorial-01/)\\\\n* [허깅페이스(HuggingFace) 모델 사용법](https://teddylee777.github.io/langchain/langchain-tutorial-02/)\\\\n* [챗(chat) - ConversationChain, 템플릿 사용법](https://teddylee777.github.io/langchain/langchain-tutorial-03/)\\\\n* [정형데이터(CSV, Excel) - ChatGPT 기반 데이터분석](https://teddylee777.github.io/langchain/langchain-tutorial-04/)\\\\n* [웹사이트 크롤링 - 웹사이트 문서 요약](https://teddylee777.github.io/langchain/langchain-tutorial-05/)\\\\n* [웹사이트 정보 추출 - 스키마 활용법](https://teddylee777.github.io/langchain/langchain-tutorial-06/)\\\\n* [PDF 문서요약, Map-Reduce](https://teddylee777.github.io/langchain/langchain-tutorial-07/)\\\\n* [PDF 기반 질의응답(Question-Answering)](https://teddylee777.github.io/langchain/langchain-tutorial-08/)\\\\n* [문장을 파이썬 코드로, 이미지를 파이썬 코드로 변경하는 방법](https://teddylee777.github.io/langchain/langchain-code-generator/)\\\\n* [LangChain Expression Language(LCEL) 원리 이해와 파이프라인 구축 가이드](https://teddylee777.github.io/langchain/langchain-lcel/)\\\\n* [LLMs를 활용한 문서 요약 가이드: Stuff, Map-Reduce, Refine 방법 총정리](https://teddylee777.github.io/langchain/summarize-chain/)\\\\n* [자동화된 메타데이터 태깅으로 문서의 메타데이터(metadata) 생성 및 자동 라벨링](https://teddylee777.github.io/langchain/metadata-tagger/)\\\\n* [네이버 뉴스 기반 Q&A 애플리케이션 구축하기 - 기본편](https://teddylee777.github.io/langchain/rag-naver-news-qa/)\\\\n* [RAG 파헤치기: 문서 기반 QA 시스템 설계 방법 - 심화편](https://teddylee777.github.io/langchain/rag-tutorial/)\\\\n* [에이전트(Agent)와 도구(tools)를 활용한 지능형 검색 시스템 구축 가이드](https://teddylee777.github.io/langchain/langchain-agent/)\\\\n\\\\n### LangGraph\\\\n\\\\n* [Multi-Agent Collaboration(다중 협업 에이전트) 로 복잡한 테스크를 수행하는 LLM 어플리케이션 제작](https://teddylee777.github.io/langgraph/langgraph-multi-agent-collaboration/)\\\\n* [LangGraph Retrieval Agent를 활용한 동적 문서 검색 및 처리](https://teddylee777.github.io/langgraph/langgraph-agentic-rag/)\\\\n\\\\n## 👥 LangChain 밋업 2024 Q1 발표자료\\\\n\\\\n* [RAG - 우리가 절대 쉽게 원하는 결과물을 얻을 수 없는 이유 - 테디노트](https://aifactory.space/task/2719/discussion/830)\\\\n* [프름프트 흐름과 LLM 모델 평가 - 이재석님](https://aifactory.space/task/2719/discussion/831)\\\\n* [인공지능을 통한 게임 제작 파이프라인의 변화 - 김한얼님](https://aifactory.space/task/2719/discussion/834)\\\\n* [OpenAI SORA 살짝 맛보기 - 박정현님](https://aifactory.space/task/2719/discussion/839)\\\\n* [Semantic Kernel로 만드는 AI Copilot - 이종인님](https://aifactory.space/task/2719/discussion/835)\\\\n* [Streamlit 과 langchain으로 나만의 웹서비스 개발하기 - 최재혁님](https://aifactory.space/task/2719/discussion/832)\\\\n* [Llama2-koen을 만들기까지 - 최태균님](https://aifactory.space/task/2719/discussion/836)\\\\n* [올바른 한국어 언어 모델 평가를 위해: HAE-RAE Bench, KMMLU - 손규진님](https://aifactory.space/task/2719/discussion/833)\\\\n* [랭체인 네이버 기사 크롤링 - 우성우님](https://aifactory.space/task/2719/discussion/829)\\\\n* [Gemma와 LangChain을 이용한 SQL 체인만들기 - 김태영님](https://aifactory.space/task/2719/discussion/841)\\\\n\\\\n## 📜 라이선스\\\\n\\\\n본 프로젝트는 [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0)에 따라 라이선스가 부여됩니다.\\\\n\\\\n### 🚫 라이선스 고지\\\\n\\\\n🔒 본 내용의 저작권은 2024년 [테디노트](https://teddylee777.github.io)에 있습니다. 모든 권리는 저작권자에게 있으며, [teddylee777@gmail.com](mailto:teddylee777@gmail.com) 으로 문의할 수 있습니다.\\\\n\\\\n```\\\\nCopyright 2024 테디노트(teddylee777@gmail.com) Licensed under the Apache License, Version 2.0 (the \\\\\"License\\\\\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\\\"AS IS\\\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. \\\\n```\\\\n\\\\n**인용 및 출처 표기**\\\\n\\\\n* 본 저작물의 내용을 블로그, 유튜브 등 온라인 매체에 인용하여 게재하는 경우, 저작권법에 따라 반드시 **출처를 명시** 해야 합니다.\\\\n\\\\n**상업적 사용에 대한 사전 협의**\\\\n\\\\n* 본 저작물(Wikidocs 및 관련 실습 코드 포함)을 강의, 강연 등 **상업적 목적으로 활용하고자 하는 경우**, 저작권자와의 사전 서면 협의가 필수적으로 요구됩니다.\\\\n\\\\n본 내용의 무단 전재 및 재배포를 금지합니다. 본 내용의 전체 혹은 일부를 인용할 경우, 출처를 명확히 밝혀주시기 바랍니다. 본 문서는 다른 문서의 내용을 참고하여 작성되었을 수 있습니다. 참고 자료는 본 문서 하단의 출처 목록에서 확인하실 수 있습니다.\\\\n\\\\n## 📚 출처\\\\n\\\\n* [langchain-ai](https://github.com/langchain-ai/langchain) 📖\\\\n* [OpenAI API Reference](https://platform.openai.com/docs/introduction) 🤖\\\\n\\\\n## 🌐 추가 자료\\\\n\\\\n* **유튜브 채널**: [LangChain 한국어 튜토리얼](https://www.youtube.com/channel/UCt2wAAXgm87ACiQnDHQEW6Q) 🎥\\\\n* **블로그**: [테디노트](https://teddylee777.github.io) 📝\\\\n* **Playground**: [LangChain LLM Playground](http://llm.teddynote.com) 🎮\\\\n\\\\n## 🚀 시작하기\\\\n\\\\n본 튜토리얼을 시작하기 전에, LangChain과 관련된 기본적인 지식을 갖추는 것이 좋습니다. 위의 출처 링크를 통해 기본적인 정보를 얻을 수 있습니다.\\\\n\\\\n## Start History\\\\n\\\\n## 💡 컨트리뷰션\\\\n\\\\n본 튜토리얼에 기여하고자 하는 분들은 언제든지 풀 리퀘스트를 보내주시거나, 이슈를 등록하여 의견을 공유해 주시기 바랍니다. 모든 기여는 본 프로젝트의 발전에 큰 도움이 됩니다. 💖\\\\n\\\\n## About\\\\n\\\\nLangChain 공식 Document, Cookbook, 그 밖의 실용 예제를 바탕으로 작성한 한국어 튜토리얼입니다. 본 튜토리얼을 통해 LangChain을 더 쉽고 효과적으로 사용하는 방법을 배울 수 있습니다.\\\\n\\\\n[wikidocs.net/book/14314](https://wikidocs.net/book/14314 \\\\\"https://wikidocs.net/book/14314\\\\\")\\\\n\\\\n### Topics\\\\n\\\\n[tutorial](/topics/tutorial \\\\\"Topic: tutorial\\\\\")   [cookbook](/topics/cookbook \\\\\"Topic: cookbook\\\\\")   [openai](/topics/openai \\\\\"Topic: openai\\\\\")   [huggingface](/topics/huggingface \\\\\"Topic: huggingface\\\\\")   [gpt-3](/topics/gpt-3 \\\\\"Topic: gpt-3\\\\\")   [openai-api](/topics/openai-api \\\\\"Topic: openai-api\\\\\")   [gpt-4](/topics/gpt-4 \\\\\"Topic: gpt-4\\\\\")   [generative-ai](/topics/generative-ai \\\\\"Topic: generative-ai\\\\\")   [chatgpt](/topics/chatgpt \\\\\"Topic: chatgpt\\\\\")   [langchain](/topics/langchain \\\\\"Topic: langchain\\\\\")   [chatgpt-api](/topics/chatgpt-api \\\\\"Topic: chatgpt-api\\\\\")   [langchain-python](/topics/langchain-python \\\\\"Topic: langchain-python\\\\\")\\\\n\\\\n### Resources\\\\n\\\\n### License\\\\n\\\\n[Apache-2.0 license](#Apache-2.0-1-ov-file)\\\\n\\\\n### Uh oh!\\\\n\\\\nThere was an error while loading. Please reload this page.\\\\n\\\\n### Stars\\\\n\\\\n[**1.8k** stars](/teddylee777/langchain-kr/stargazers)\\\\n\\\\n### Watchers\\\\n\\\\n[**41** watching](/teddylee777/langchain-kr/watchers)\\\\n\\\\n### Forks\\\\n\\\\n[**648** forks](/teddylee777/langchain-kr/forks)\\\\n\\\\n[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2Fteddylee777%2Flangchain-kr&report=teddylee777+%28user%29)\\\\n\\\\n## [Releases](/teddylee777/langchain-kr/releases)\\\\n\\\\nNo releases published\\\\n\\\\n## [Packages 0](/users/teddylee777/packages?repo_name=langchain-kr)\\\\n\\\\nNo packages published\\\\n\\\\n### Uh oh!\\\\n\\\\nThere was an error while loading. Please reload this page.\\\\n\\\\n## [Contributors 5](/teddylee777/langchain-kr/graphs/contributors)\\\\n\\\\n## Languages\\\\n\\\\n* [Jupyter Notebook 97.8%](/teddylee777/langchain-kr/search?l=jupyter-notebook)\\\\n* [Python 2.0%](/teddylee777/langchain-kr/search?l=python)\\\\n* [HTML 0.2%](/teddylee777/langchain-kr/search?l=html)\\\\n\\\\nYou can’t perform that action at this time.\\\\n\\\\n \"}, {\"url\": \"https://wikidocs.net/book/14314\", \"title\": \"<랭체인LangChain 노트> - LangChain 한국어 튜토리얼 - 위키독스\", \"content\": \"CH01 LangChain 시작하기) CH02 프롬프트(Prompt)) CH03 출력 파서(Output Parsers)) CH04 모델(Model)) CH05 메모리(Memory)) LCEL Chain 에 메모리 추가) CH06 문서 로더(Document Loader)) CH07 텍스트 분할(Text Splitter)) CH08 임베딩(Embedding)) CH09 벡터저장소(VectorStore)) CH10 검색기(Retriever)) CH11 리랭커(Reranker)) CH12 Retrieval Augmented Generation(RAG)) CH13 LangChain Expression Language(LCEL)) CH14 체인(Chains)) CH15 평가(Evaluations)) CH16 에이전트(Agent)) 에이전트(Agent)) Claude, Gemini, Ollama, Together.ai 를 활용한 Agent) Agentic RAG) RAG + Image Generator Agent(보고서 작성)) 도구를 활용한 토론 에이전트(Two Agent Debates with Tools)) CH17 LangGraph) Agent 에 메모리(memory) 추가) Agentic RAG) 사용자 요구사항 기반 메타 프롬프트 생성 에이전트) 참고 자료는 본 문서 하단의 출처 목록에서 확인하실 수 있습니다. def execute\\\\\\\\_agent(llm, tools, input\\\\\\\\_text, label): 에 label이 어디쓰이는지 누락되어 있네요 CH09 벡터저장소(VectorStore) CH01 LangChain 시작하기 * **다음글** : CH01 LangChain 시작하기\", \"score\": 0.7587435, \"raw_content\": \"[**<랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷**](/book/14314)\\\\n[CH01 LangChain 시작하기](javascript:page(233341))\\\\n[01. 설치 영상보고 따라하기](javascript:page(257836))\\\\n[02. OpenAI API 키 발급 및 테스트](javascript:page(233342))\\\\n[03. LangSmith 추적 설정](javascript:page(250954))\\\\n[04. OpenAI API 사용(GPT-4o 멀티모달)](javascript:page(233343))\\\\n[05. LangChain Expression Language(LCEL)](javascript:page(233344))\\\\n[06. LCEL 인터페이스](javascript:page(233345))\\\\n[07. Runnable](javascript:page(233346))\\\\n[CH02 프롬프트(Prompt)](javascript:page(233347))\\\\n[01. 프롬프트(Prompt)](javascript:page(233351))\\\\n[02. 퓨샷 프롬프트(FewShotPromptTemplate)](javascript:page(233348))\\\\n[03. LangChain Hub](javascript:page(233349))\\\\n[04. 개인화된 프롬프트(Hub에 업로드)](javascript:page(233350))\\\\n[CH03 출력 파서(Output Parsers)](javascript:page(233771))\\\\n[01. Pydantic 출력 파서(PydanticOutputParser)](javascript:page(233786))\\\\n[02. 콤마 구분자 출력 파서(CommaSeparatedListOutputParser)](javascript:page(233787))\\\\n[03. 구조화된 출력 파서(StructuredOuputParser)](javascript:page(233788))\\\\n[04. JSON 출력 파서(JsonOutputParser)](javascript:page(233789))\\\\n[05. 데이터프레임 출력 파서(PandasDataFrameOutputParser)](javascript:page(233790))\\\\n[06. 날짜 형식 출력 파서(DatetimeOutputParser)](javascript:page(233791))\\\\n[07. 열거형 출력 파서(EnumOutputParser)](javascript:page(233792))\\\\n[08. 출력 수정 파서(OutputFixingParser)](javascript:page(233793))\\\\n[CH04 모델(Model)](javascript:page(233772))\\\\n[01. 다양한 LLM 모델 활용](javascript:page(233795))\\\\n[02. 캐싱(Cache)](javascript:page(233796))\\\\n[03. 모델 직렬화(Serialization) - 저장 및 불러오기](javascript:page(233798))\\\\n[04. 토큰 사용량 확인](javascript:page(233797))\\\\n[05. 구글 생성 AI(Google Generative AI)](javascript:page(233799))\\\\n[06. 허깅페이스 엔드포인트(HuggingFace Endpoints)](javascript:page(233802))\\\\n[07. 허깅페이스 로컬(HuggingFace Local)](javascript:page(233803))\\\\n[08. 허깅페이스 파이프라인(HuggingFace Pipeline)](javascript:page(233804))\\\\n[09. 올라마(Ollama)](javascript:page(233805))\\\\n[10. GPT4ALL](javascript:page(233806))\\\\n[11. 비디오(Video) 질의 응답 LLM (Gemini)](javascript:page(263314))\\\\n[CH05 메모리(Memory)](javascript:page(233773))\\\\n[01. 대화 버퍼 메모리(ConversationBufferMemory)](javascript:page(233801))\\\\n[02. 대화 버퍼 윈도우 메모리(ConversationBufferWindowMemory)](javascript:page(233800))\\\\n[03. 대화 토큰 버퍼 메모리(ConversationTokenBufferMemory)](javascript:page(233807))\\\\n[04. 대화 엔티티 메모리(ConversationEntityMemory)](javascript:page(233808))\\\\n[05. 대화 지식그래프 메모리(ConversationKGMemory)](javascript:page(233809))\\\\n[06. 대화 요약 메모리(ConversationSummaryMemory)](javascript:page(233810))\\\\n[07. 벡터저장소 검색 메모리(VectorStoreRetrieverMemory)](javascript:page(233811))\\\\n[08. LCEL Chain 에 메모리 추가](javascript:page(233812))\\\\n[09. SQLite 에 대화내용 저장](javascript:page(233813))\\\\n[10. RunnableWithMessageHistory에 ChatMessageHistory추가](javascript:page(254682))\\\\n[CH06 문서 로더(Document Loader)](javascript:page(233775))\\\\n[01. 도큐먼트(Document) 의 구조](javascript:page(253706))\\\\n[02. PDF](javascript:page(253707))\\\\n[03. 한글(HWP)](javascript:page(253708))\\\\n[04. CSV](javascript:page(253709))\\\\n[05. Excel](javascript:page(253710))\\\\n[06. Word](javascript:page(253711))\\\\n[07. PowerPoint](javascript:page(253712))\\\\n[08. 웹 문서(WebBaseLoader)](javascript:page(253713))\\\\n[09. 텍스트(TextLoader)](javascript:page(253714))\\\\n[10. JSON](javascript:page(253715))\\\\n[11. Arxiv](javascript:page(253716))\\\\n[12. UpstageLayoutAnalysisLoader](javascript:page(253717))\\\\n[13. LlamaParser](javascript:page(253718))\\\\n[CH07 텍스트 분할(Text Splitter)](javascript:page(233776))\\\\n[01. 문자 텍스트 분할(CharacterTextSplitter)](javascript:page(233998))\\\\n[02. 재귀적 문자 텍스트 분할(RecursiveCharacterTextSplitter)](javascript:page(233999))\\\\n[03. 토큰 텍스트 분할(TokenTextSplitter)](javascript:page(234002))\\\\n[04. 시멘틱 청커(SemanticChunker)](javascript:page(234003))\\\\n[05. 코드 분할(Python, Markdown, JAVA, C++, C#, GO, JS, Latex 등)](javascript:page(234004))\\\\n[06. 마크다운 헤더 텍스트 분할(MarkdownHeaderTextSplitter)](javascript:page(234005))\\\\n[07. HTML 헤더 텍스트 분할(HTMLHeaderTextSplitter)](javascript:page(234006))\\\\n[08. 재귀적 JSON 분할(RecursiveJsonSplitter)](javascript:page(234007))\\\\n[CH08 임베딩(Embedding)](javascript:page(233777))\\\\n[01. OpenAIEmbeddings](javascript:page(233815))\\\\n[02. 캐시 임베딩(CacheBackedEmbeddings)](javascript:page(233816))\\\\n[03. 허깅페이스 임베딩(HuggingFace Embeddings)](javascript:page(233817))\\\\n[04. UpstageEmbeddings](javascript:page(253106))\\\\n[05. OllamaEmbeddings](javascript:page(253107))\\\\n[06. GPT4ALL 임베딩](javascript:page(233818))\\\\n[07. Llama CPP 임베딩](javascript:page(233819))\\\\n[CH09 벡터저장소(VectorStore)](javascript:page(233778))\\\\n[01. Chroma](javascript:page(234094))\\\\n[02. FAISS](javascript:page(234014))\\\\n[03. Pinecone](javascript:page(252407))\\\\n[CH10 검색기(Retriever)](javascript:page(233779))\\\\n[01. 벡터스토어 기반 검색기(VectorStore-backed Retriever)](javascript:page(234016))\\\\n[02. 문맥 압축 검색기(ContextualCompressionRetriever)](javascript:page(234097))\\\\n[03. 앙상블 검색기(EnsembleRetriever)](javascript:page(234100))\\\\n[04. 긴 문맥 재정렬(LongContextReorder)](javascript:page(234101))\\\\n[05. 상위 문서 검색기(ParentDocumentRetriever)](javascript:page(234164))\\\\n[06. 다중 쿼리 검색기(MultiQueryRetriever)](javascript:page(234109))\\\\n[07. 다중 벡터저장소 검색기(MultiVectorRetriever)](javascript:page(234281))\\\\n[08. 셀프 쿼리 검색기(SelfQueryRetriever)](javascript:page(234475))\\\\n[09. 시간 가중 벡터저장소 검색기(TimeWeightedVectorStoreRetriever)](javascript:page(234604))\\\\n[10. 한글 형태소 분석기(Kiwi, Kkma, Okt) + BM25 검색기](javascript:page(251980))\\\\n[11. Convex Combination(CC) 적용된 앙상블 검색기(EnsembleRetriever)](javascript:page(263833))\\\\n[CH11 리랭커(Reranker)](javascript:page(253434))\\\\n[01. Cross Encoder Reranker](javascript:page(253836))\\\\n[02. Cohere Reranker](javascript:page(253837))\\\\n[03. Jina Reranker](javascript:page(253838))\\\\n[04. FlashRank Reranker](javascript:page(253839))\\\\n[CH12 Retrieval Augmented Generation(RAG)](javascript:page(233780))\\\\n[01. PDF 문서 기반 QA(Question-Answer)](javascript:page(251190))\\\\n[02. 네이버 뉴스기사 QA(Question-Answer)](javascript:page(234008))\\\\n[03. RAG 의 기능별 다양한 모듈 활용기](javascript:page(234009))\\\\n[04. RAPTOR: 긴 문맥 요약(Long Context Summary)](javascript:page(234017))\\\\n[05. 대화내용을 기억하는 RAG 체인](javascript:page(252858))\\\\n[CH13 LangChain Expression Language(LCEL)](javascript:page(233781))\\\\n[01. RunnablePassthrough](javascript:page(235580))\\\\n[02. Runnable 구조(그래프) 검토](javascript:page(235884))\\\\n[03. RunnableLambda](javascript:page(235705))\\\\n[04. LLM 체인 라우팅(RunnableLambda, RunnableBranch)](javascript:page(235882))\\\\n[05. RunnableParallel](javascript:page(235883))\\\\n[06. 동적 속성 지정(configurable\\\\\\\\_fields, configurable\\\\\\\\_alternatives)](javascript:page(235704))\\\\n[07. @chain 데코레이터로 Runnable 구성](javascript:page(235703))\\\\n[08. RunnableWithMessageHistory](javascript:page(235581))\\\\n[09. 사용자 정의 제네레이터(generator)](javascript:page(235885))\\\\n[10. Runtime Arguments 바인딩](javascript:page(235886))\\\\n[11. 폴백(fallback) 모델 지정](javascript:page(235938))\\\\n[CH14 체인(Chains)](javascript:page(233774))\\\\n[01. 문서 요약](javascript:page(234020))\\\\n[02. SQL](javascript:page(234019))\\\\n[03. 구조화된 출력 체인(with\\\\\\\\_structered\\\\\\\\_output)](javascript:page(256983))\\\\n[CH15 평가(Evaluations)](javascript:page(259203))\\\\n[01. 합성 테스트 데이터셋 생성(RAGAS)](javascript:page(259204))\\\\n[02. RAGAS 를 활용한 평가](javascript:page(259205))\\\\n[03. 생성한 평가용 데이터셋 업로드(HuggingFace Dataset)](javascript:page(259206))\\\\n[04. LangSmith 데이터셋 생성](javascript:page(259207))\\\\n[05. LLM-as-Judge](javascript:page(259208))\\\\n[06. 임베딩 기반 평가(embedding\\\\\\\\_distance)](javascript:page(259210))\\\\n[07. 사용자 정의(Custom) LLM 평가](javascript:page(259212))\\\\n[08. Rouge, BLEU, METEOR, SemScore 기반 휴리스틱 평가](javascript:page(259213))\\\\n[09. 실험(Experiment) 평가 비교](javascript:page(259214))\\\\n[10. 요약(Summary) 방식의 평가](javascript:page(259215))\\\\n[11. Groundedness(할루시네이션) 평가](javascript:page(259216))\\\\n[12. 실험 비교(Pairwise Evaluation)](javascript:page(259217))\\\\n[13. 반복 평가](javascript:page(259218))\\\\n[14. 온라인 평가를 활용한 평가 자동화](javascript:page(259219))\\\\n[CH16 에이전트(Agent)](javascript:page(233782))\\\\n[01. 도구(Tools)](javascript:page(262582))\\\\n[02. 도구 바인딩(Binding Tools)](javascript:page(262585))\\\\n[03. 에이전트(Agent)](javascript:page(262586))\\\\n[04. Claude, Gemini, Ollama, Together.ai 를 활용한 Agent](javascript:page(262592))\\\\n[05. Iteration 기능과 사람 개입(Human-in-the-loop)](javascript:page(262593))\\\\n[06. Agentic RAG](javascript:page(262595))\\\\n[07. CSVExcel 데이터 분석 Agent](javascript:page(262597))\\\\n[08. Toolkits 활용 Agent](javascript:page(262604))\\\\n[09. RAG + Image Generator Agent(보고서 작성)](javascript:page(262612))\\\\n[10. 도구를 활용한 토론 에이전트(Two Agent Debates with Tools)](javascript:page(234162))\\\\n[CH17 LangGraph](javascript:page(233785))\\\\n[01. 핵심 기능](javascript:page(265670))\\\\n[01. LangGraph 에 자주 등장하는 Python 문법이해](javascript:page(264613))\\\\n[02. LangGraph를 활용한 챗봇 구축](javascript:page(264614))\\\\n[03. LangGraph를 활용한 Agent 구축](javascript:page(264624))\\\\n[04. Agent 에 메모리(memory) 추가](javascript:page(265658))\\\\n[05. 노드의 단계별 스트리밍 출력](javascript:page(265659))\\\\n[06. Human-in-the-loop(사람의 개입)](javascript:page(265663))\\\\n[07. 중간단계 개입 되돌림을 통한 상태 수정과 Replay](javascript:page(265723))\\\\n[08. 사람(Human)에게 물어보는 노드 추가](javascript:page(265737))\\\\n[09. 메시지 삭제(RemoveMessage)](javascript:page(265749))\\\\n[10. ToolNode 를 사용하여 도구를 호출하는 방법](javascript:page(265763))\\\\n[11. 병렬 노드 실행을 위한 분기 생성 방법](javascript:page(265766))\\\\n[12. 대화 기록 요약을 추가하는 방법](javascript:page(265767))\\\\n[13. 서브그래프 추가 및 사용 방법](javascript:page(265768))\\\\n[14. 서브그래프의 입력과 출력을 변환하는 방법](javascript:page(265769))\\\\n[15. LangGraph 스트리밍 모드의 모든 것](javascript:page(265770))\\\\n[02. 구조 설계](javascript:page(267807))\\\\n[01. 기본 그래프 생성](javascript:page(267808))\\\\n[02. Naive RAG](javascript:page(267809))\\\\n[03. 관련성 체커(Relevance Checker) 모듈 추가](javascript:page(267810))\\\\n[04. 웹 검색 모듈 추가](javascript:page(267811))\\\\n[05. 쿼리 재작성 모듈 추가](javascript:page(267812))\\\\n[06. Agentic RAG](javascript:page(267813))\\\\n[07. Adaptive RAG](javascript:page(267814))\\\\n[03. Use Cases](javascript:page(267815))\\\\n[01. 에이전트 대화 시뮬레이션 (고객 응대 시나리오)](javascript:page(267816))\\\\n[02. 사용자 요구사항 기반 메타 프롬프트 생성 에이전트](javascript:page(267817))\\\\n[03. CRAG(Corrective RAG)](javascript:page(270686))\\\\n[04. Self-RAG](javascript:page(270687))\\\\n[05. 계획 후 실행(Plan-and-Execute)](javascript:page(270688))\\\\n[06. 멀티 에이전트 협업 네트워크(Multi-Agent Collaboration Network)](javascript:page(270689))\\\\n[07. 멀티 에이전트 감독자(Multi-Agent Supervisor)](javascript:page(270690))\\\\n[08. 계층적 멀티 에이전트 팀(Hierarchical Multi-Agent Teams)](javascript:page(270691))\\\\n[09. SQL 데이터베이스와 상호작용하는 에이전트](javascript:page(270692))\\\\n[10. STORM 개념을 도입한 연구를 위한 멀티 에이전트](javascript:page(270693))\\\\n[CH18 기타 정보](javascript:page(265575))\\\\n[01. StreamEvent 타입별 정리](javascript:page(265576))\\\\n\\\\n1. [<랭체인LangChain 노트> - Lang…](/book/14314)\\\\n\\\\n1. [위키독스](/)\\\\n\\\\n# <랭체인LangChain 노트> - LangChain 한국어 튜토리얼🇰🇷\\\\n\\\\n지은이:\\\\n[테디노트](/profile/info/book/10226)\\\\n\\\\n최종 편집일시 :\\\\n2025년 4월 30일 8:06 오후\\\\n\\\\n저작권 :\\\\n\\\\n[**3,341** 명이 추천](javascript:recommend_book(\\'14314\\'); \\\\\"추천\\\\\")\\\\n\\\\n**추천**은 공유할 수 있는 무료 전자책을 집필하는데 정말 큰 힘이 됩니다. **\\\\\"추천\\\\\"** 한 번씩만 부탁 드리겠습니다🙏🙏\\\\n\\\\n✅ **랭체인 한국어 튜토리얼 강의**   \\\\n[패스트캠퍼스 - RAG 비법노트](https://fastcampus.co.kr/data_online_teddy?utm_source=wikidocs&utm_medium=viral&utm_campaign=prd%5E250318%5E239355&utm_content=teacher%5E239355)\\\\n\\\\n✅ **랭체인 한국어 튜토리얼 코드저장소(GitHub)** 📘🖥️  \\\\n<https://github.com/teddylee777/langchain-kr>\\\\n\\\\n✅ **유튜브 \\\\\"테디노트\\\\\"** 🎥📚  \\\\n<https://www.youtube.com/c/@teddynote>\\\\n\\\\n✅ **데이터 분석 블로그**\\\\n<https://teddylee777.github.io>\\\\n\\\\n✅ **문의**\\\\n[teddylee777@gmail.com](mailto:teddylee777@gmail.com)\\\\n\\\\n### LICENSE\\\\n\\\\n**인용 및 출처 표기**\\\\n\\\\n* 본 저작물을 블로그, 유튜브 등 온라인 매체에 인용하여 게재할 경우, **Creative Commons Attribution-NonCommercial-NoDerivs 2.0 Korea** 라이선스에 따라 **반드시 출처를 명시**해야 합니다.\\\\n\\\\n**상업적 사용에 대한 사전 협의**\\\\n\\\\n* 본 저작물(Wikidocs 및 관련 실습 코드 포함)을 **강의, 강연 등 상업적 목적으로 활용하고자 하는 경우**, **저작권자와의 사전 서면 협의가 필수적으로 요구**됩니다. 해당 협의는 teddylee777@gmail.com으로 문의하여 진행하실 수 있습니다.\\\\n\\\\n```\\\\n본 저작물은 2025년 테디노트에 의해 작성되었습니다. \\\\n모든 권리는 저작권자에게 있으며, 본 저작물은 Creative Commons Attribution-NonCommercial-NoDerivs 2.0 Korea 라이선스에 따라 배포됩니다.\\\\n본 저작물의 무단 전재 및 재배포를 금지하며, 전체 혹은 일부를 인용할 경우 출처를 명확히 밝혀주시기 바랍니다.\\\\n본 문서는 다른 문서의 내용을 참고하여 작성되었을 수 있습니다. 참고 자료는 본 문서 하단의 출처 목록에서 확인하실 수 있습니다.\\\\nCopyright (c) 테디노트.\\\\n\\\\n```\\\\n\\\\n### Reference\\\\n\\\\n* [LangChain Github](https://github.com/langchain-ai/langchain)\\\\n* [LangGraph Github](https://github.com/langchain-ai/langgraph)\\\\n* [LangChain Document](https://python.langchain.com/docs/get_started/introduction)\\\\n\\\\n\\\\n\\\\n광고가 출력될 위치입니다.\\\\n\\\\n광고가 출력될 위치입니다.\\\\n\\\\n\\\\n[최근 댓글 (13)](javascript:show_book_comments())\\\\n[최근 수정글 (10)](javascript:show_book_pages())\\\\n [RSS](/book/14314/rss/)\\\\n\\\\n[04. Claude, Gemini, Ollama, Together.ai 를 활용한 Agent](/262592#comment_27015)\\\\n\\\\n-\\\\n[동구](/profile/info/book/11043),\\\\n2025년 7월 18일 1:53 오후\\\\n\\\\ndef execute\\\\\\\\_agent(llm, tools, input\\\\\\\\_text, label): 에 label이 어디쓰이는지 누락되어 있네요\\\\n\\\\n[02. RAGAS 를 활용한 평가](/259205#comment_26817)\\\\n\\\\n-\\\\n[동구](/profile/info/book/11043),\\\\n2025년 7월 8일 6:47 오후\\\\n\\\\nanswer relevancy에는 context가 안쓰이는거 같은데요\\\\n\\\\n[09. 올라마(Ollama)](/233805#comment_26344)\\\\n\\\\n-\\\\n[dongman kim](/profile/info/book/31305),\\\\n2025년 6월 12일 2:35 오후\\\\n\\\\nfrom langchain\\\\\\\\_community.chat\\\\\\\\_models import ChatOllama 가 이전 버전이고 from langchain\\\\\\\\_ollama import ChatOllama 로 변경되었는데 아직 반영안된 부분이 있습니다. 수정바랍니다\\\\n\\\\n[12. UpstageLayoutAnalysisLoader](/253717#comment_25299)\\\\n\\\\n-\\\\n[천세현](/profile/info/book/30075),\\\\n2025년 3월 29일 9:46 오전\\\\n\\\\n안녕하세요!langchain-upstage 0.6.0 버젼에서 from langchain\\\\\\\\_upstage import UpstageLayoutAnalysisLoader 할 때 ImportError이 발생하는데 왜 그런가요?\\\\n\\\\n[CH09 벡터저장소(VectorStore)](/233778#comment_24275)\\\\n\\\\n-\\\\n[이서혁](/profile/info/book/29233),\\\\n2025년 2월 13일 4:57 오후\\\\n\\\\n안녕하세요 :) 정말 알찬 자료를 전자책으로 풀어주셔서 감사합니다. 드리고자하는 질문은, 다른 자료들을 봐도 테디노트님은 VectorDB 중 Milvus는 다루시지 않던데 그 이유가 궁금합니다. 혹시 특별한 이유가 있을까요?\\\\n\\\\n[02. 네이버 뉴스기사 QA(Question-Answer)](/234008#comment_24121)\\\\n\\\\n-\\\\n[김민겸](/profile/info/book/29073),\\\\n2025년 2월 2일 12:17 오후\\\\n\\\\n\\\\\"bullet points 형식으로 정리\\\\\"에서 \\\\\"주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다.\\\\\" 라고 나오는데 이유를 알려주실 수 있나요? [kmk582@naver.com](mailto:kmk582@naver.com)\\\\n\\\\n[10. 한글 형태소 분석기(Kiwi, Kkma, Okt) + BM25 검색기](/251980#comment_23499)\\\\n\\\\n-\\\\n[shcheon99@naver.com](/profile/info/book/28627),\\\\n2025년 1월 9일 12:28 오후\\\\n\\\\n출력된 결과를 비교했을 때, kiwi tokenizer을 사용한 결과와 kkma, okt 를 사용한 결과가 큰 차이가 없다고 봐도 되는 건가요?\\\\n\\\\n[CH01 LangChain 시작하기](/233341#comment_22761)\\\\n\\\\n-\\\\n[NamHyeon](/profile/info/book/14059),\\\\n2024년 12월 8일 1:17 오후\\\\n\\\\n좋은 자료를 무료로 공유해 주셔서, 감사한 마음에 \\'테디노트의 RAG 비법노트\\' 강의 등록했습니다 ! 물론 제 현업에 필요한 기술이라서, 강의 또한 기쁜 마음에 신청했구요 ~ 정주행 해서, 창공을 날아가 보겠습니다 ^^\\\\n\\\\n[06. Word](/253711#comment_21963)\\\\n\\\\n-\\\\n[Paul](/profile/info/book/27425),\\\\n2024년 10월 27일 5:38 오후\\\\n\\\\npython-docx도 설치해야 할까요?\\\\n\\\\n[10. JSON](/253715#comment_21962)\\\\n\\\\n-\\\\n[Paul](/profile/info/book/27425),\\\\n2024년 10월 27일 5:37 오후\\\\n\\\\n!pip install jq 부분이 들어가야 할 것 같습니다.\\\\n\\\\n[02. PDF](/253707#comment_21954)\\\\n\\\\n-\\\\n[Paul](/profile/info/book/27425),\\\\n2024년 10월 27일 3:29 오후\\\\n\\\\n<html><head> <meta http-equiv=\\\\\"Content-Type\\\\\" content=\\\\\"text/html\\\\\"> </head><body> <span style=\\\\\"position:absolute; border: gray 1px solid; left:0px; top:50px; width:612px; height:858px;\\\\\"></span> <div style=\\\\\"position:absolute; top:50px;\\\\\"><a name=\\\\\"1\\\\\">Page 1</a></div> <div style=\\\\\"position:absolute; border 이 부분이 출력 결과가 아니라 코드인 것처럼 표시되어 있네요~\\\\n\\\\n[12. UpstageLayoutAnalysisLoader](/253717#comment_21944)\\\\n\\\\n-\\\\n[Paul](/profile/info/book/27425),\\\\n2024년 10월 27일 10:59 오전\\\\n\\\\n감사히 잘 참고하고 있습니다. 아주 사소한 오기이지만... 11번 Arxiv 다음에 12번이 와야 할 텐데, 원래 넣으시려던 다른 목차가 빠진 것인지 바로 13번이 나왔네요^^\\\\n\\\\n[03. 모델 직렬화(Serialization) - 저장 및 불러오기](/233798#comment_21094)\\\\n\\\\n-\\\\n[동구](/profile/info/book/11043),\\\\n2024년 9월 20일 12:58 오후\\\\n\\\\nloads는 뭐에요?\\\\n\\\\n[06. Human-in-the-loop(사람의 개입)](/265663)\\\\n\\\\n-\\\\n2025년 4월 14일 7:10 오후\\\\n\\\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\\\n\\\\n[15. LangGraph 스트리밍 모드의 모든 것](/265770)\\\\n\\\\n-\\\\n2025년 3월 23일 11:48 오후\\\\n\\\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\\\n\\\\n[10. JSON](/253715)\\\\n\\\\n-\\\\n2025년 1월 16일 12:23 오전\\\\n\\\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\\\n\\\\n[03. 대화 토큰 버퍼 메모리(ConversationTokenBufferMemory)](/233807)\\\\n\\\\n-\\\\n2025년 1월 16일 12:23 오전\\\\n\\\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\\\n\\\\n[05. 코드 분할(Python, Markdown, JAVA, C++, C#, GO, JS, Latex 등)](/234004)\\\\n\\\\n-\\\\n2025년 1월 16일 12:19 오전\\\\n\\\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\\\n\\\\n[04. Self-RAG](/270687)\\\\n\\\\n-\\\\n2024년 12월 23일 3:48 오전\\\\n\\\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\\\n\\\\n[10. STORM 개념을 도입한 연구를 위한 멀티 에이전트](/270693)\\\\n\\\\n-\\\\n2024년 12월 23일 3:16 오전\\\\n\\\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\\\n\\\\n[03. CRAG(Corrective RAG)](/270686)\\\\n\\\\n-\\\\n2024년 12월 23일 3:04 오전\\\\n\\\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\\\n\\\\n[05. 계획 후 실행(Plan-and-Execute)](/270688)\\\\n\\\\n-\\\\n2024년 12월 23일 3:04 오전\\\\n\\\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\\\n\\\\n[07. 멀티 에이전트 감독자(Multi-Agent Supervisor)](/270690)\\\\n\\\\n-\\\\n2024년 12월 23일 3:04 오전\\\\n\\\\n<style> .custom { background-color: #008d8d; color: white; padding: 0.25em 0.5e…\\\\n\\\\n* **다음글** : [CH01 LangChain 시작하기](/233341)\\\\n\\\\n같이 보면 좋은 책\\\\n\\\\n* - [Deep Learning Bible - H. Traditional NLP - Eng.](/book/9419)\\\\n* - [Deep Learning Bible - 8. Large Language Models - 한글](/book/14965)\\\\n* - [Deep Learning Bible - 8. Large Language Models - EN](/book/14966)\\\\n* - [바이브 코딩(Vibe Coding): FPS 게임 제작기](/book/17562)\\\\n* - [랭체인(LangChain) 입문부터 응용까지](/book/14473)\"}]', name='tavily_web_search', id='6ce9b93c-657e-4672-b80b-8ddca43e229b', tool_call_id='1327adf2-6963-4dc4-a9bc-09bec7f2179d'),\n",
       "  AIMessage(content='랭체인(LangChain)은 챗봇, 질의응답 시스템, 자동 요약 등 다양한 언어 모델(LLM) 애플리케이션을 쉽게 개발할 수 있도록 지원하는 프레임워크입니다. 한국어로 된 랭체인 튜토리얼은 다음과 같은 자료들이 있습니다.\\n\\n**1. 랭체인(LangChain) 한국어 튜토리얼 (위키독스 & GitHub)**\\n\\n*   **위키독스 전자책:**\\n    *   **<랭체인(LangChain) 노트> - LangChain 한국어 튜토리얼🇰🇷** 이라는 제목의 무료 전자책이 있습니다.\\n    *   LangChain의 기본적인 내용부터 시작해서 프롬프트, 출력 파서, 모델, 메모리, 문서 로더, 에이전트, LangGraph 등 매우 상세한 내용을 다루고 있습니다.\\n    *   **URL:** [https://wikidocs.net/book/14314](https://wikidocs.net/book/14314)\\n\\n*   **GitHub 저장소:**\\n    *   위키독스 전자책의 코드와 예제들을 담고 있는 GitHub 저장소입니다.\\n    *   LangChain 공식 문서, 쿡북, 그리고 다양한 실용 예제를 바탕으로 작성되었습니다.\\n    *   **URL:** [https://github.com/teddylee777/langchain-kr](https://github.com/teddylee777/langchain-kr)\\n\\n**2. 네이버 블로그**\\n\\n*   \"LangChain 공부하기 진짜진짜 좋은 자료\"라는 제목의 블로그 글에서 위에서 언급된 위키독스 튜토리얼을 추천하고 있습니다.\\n*   이 블로그에서는 LangChain을 \"개발자들이 챗봇, 질의응답 시스템, 자동 요약 등 다양한 LLM 애플리케이션을 쉽게 개발할 수 있도록 지원하는 도구모음(프레임워크)\"라고 소개하고 있습니다.\\n*   **URL:** [https://m.blog.naver.com/htk1019/223388363309](https://m.blog.naver.com/htk1019/223388363309)\\n\\n이 자료들을 통해 랭체인을 효과적으로 학습하고 활용하는 방법을 배우실 수 있습니다. 특히 위키독스 전자책과 GitHub 저장소는 매우 체계적으로 구성되어 있어 처음 시작하는 분들에게 큰 도움이 될 것입니다.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-pro', 'safety_ratings': []}, id='run--21b81b95-8bb0-4b4a-9446-61678cedf3c6-0')]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 에이전트 실행\n",
    "agent_executor.invoke(\n",
    "    {\"messages\": [(\"user\", \"랭체인 한국어 튜토리얼에 대해서 설명해줘\")]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f6efaa",
   "metadata": {},
   "source": [
    "## 상태 정의\n",
    "\n",
    "- `input`: 사용자의 입력\n",
    "- `plan`: 현재 계획\n",
    "- `past_steps`: 이전에 실행한 계획과 실행 결과\n",
    "- `response`: 최종 응답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27c44393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, Tuple\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "# 상태 정의\n",
    "class PlanExecute(TypedDict):\n",
    "    input: Annotated[str, \"User's input\"]\n",
    "    plan: Annotated[List[str], \"Current plan\"]\n",
    "    past_steps: Annotated[List[Tuple], operator.add]\n",
    "    response: Annotated[str, \"Final response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c5d026",
   "metadata": {},
   "source": [
    "## 계획(Plan) 단계\n",
    "\n",
    "이제 **계획 단계**를 생성하는 방법을 고려해 보겠습니다. 이 단계에서는 `function calling` 을 사용하여 계획을 수립합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad37fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# Plan 모델 정의\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Sorted steps to execute the plan\"\"\"\n",
    "\n",
    "    steps: Annotated[List[str], \"Different steps to follow, should be in sorted order\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "726052a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# 계획 수립을 위한 프롬프트 템플릿 생성\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"For the given objective, come up with a simple step by step plan. \\\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "Answer in Korean.\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatGoogleGenerativeAI(temperature=0, model=\"gemini-2.5-pro\")\n",
    "planner = planner_prompt | llm.with_structured_output(Plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd64cd",
   "metadata": {},
   "source": [
    "`planner` 를 실행하여 계획을 수립한 결과를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c782515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planner 실행\n",
    "planner.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"user\",\n",
    "                \"LangGraph 의 핵심 장단점과 LangGraph 를 사용하는 이유는 무엇인가?\",\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef28ddf5",
   "metadata": {},
   "source": [
    "## 재계획(Re-Plan) 단계\n",
    "\n",
    "이제 이전 단계의 결과를 바탕으로 계획을 다시 수립하는 단계를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c80e74e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "\n",
    "    # 사용자 응답\n",
    "    response: str\n",
    "\n",
    "\n",
    "class Act(BaseModel):\n",
    "    \"\"\"Action to perform.\"\"\"\n",
    "\n",
    "    # 수행할 작업: \"Response\", \"Plan\". 사용자에게 응답할 경우 Response 사용, 추가 도구 사용이 필요할 경우 Plan 사용\n",
    "    action: Union[Response, Plan] = Field(\n",
    "        description=\"Action to perform. If you want to respond to user, use Response. \"\n",
    "        \"If you need to further use tools to get the answer, use Plan.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# 계획을 재수립하기 위한 프롬프트 정의\n",
    "replanner_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"For the given objective, come up with a simple step by step plan. \\\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps. \\\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - do not skip steps.\n",
    "\n",
    "Your objective was this:\n",
    "{input}\n",
    "\n",
    "Your original plan was this:\n",
    "{plan}\n",
    "\n",
    "You have currently done the follow steps:\n",
    "{past_steps}\n",
    "\n",
    "Update your plan accordingly. If no more steps are needed and you can return to the user, then respond with that. Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Do not return previously done steps as part of the plan.\n",
    "\n",
    "Answer in Korean.\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# Replanner 생성\n",
    "replanner = replanner_prompt | ChatOpenAI(\n",
    "    model=MODEL_NAME, temperature=0\n",
    ").with_structured_output(Act)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88a7250",
   "metadata": {},
   "source": [
    "## 그래프 생성\n",
    "\n",
    "이제 그래프를 생성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df12d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# 사용자 입력을 기반으로 계획을 생성하고 반환\n",
    "def plan_step(state: PlanExecute):\n",
    "    plan = planner.invoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
    "    # 생성된 계획의 단계 리스트 반환\n",
    "    return {\"plan\": plan.steps}\n",
    "\n",
    "\n",
    "# 에이전트 실행기를 사용하여 주어진 작업을 수행하고 결과를 반환\n",
    "def execute_step(state: PlanExecute):\n",
    "    plan = state[\"plan\"]\n",
    "    # 계획을 문자열로 변환하여 각 단계에 번호를 매김\n",
    "    plan_str = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(plan))\n",
    "    task = plan[0]\n",
    "    # 현재 실행할 작업을 포맷팅하여 에이전트에 전달\n",
    "    task_formatted = f\"\"\"For the following plan:\n",
    "{plan_str}\\n\\nYou are tasked with executing [step 1. {task}].\"\"\"\n",
    "    # 에이전트 실행기를 통해 작업 수행 및 결과 수신\n",
    "    agent_response = agent_executor.invoke({\"messages\": [(\"user\", task_formatted)]})\n",
    "    # 이전 단계와 그 결과를 포함하는 딕셔너리 반환\n",
    "    return {\n",
    "        \"past_steps\": [(task, agent_response[\"messages\"][-1].content)],\n",
    "    }\n",
    "\n",
    "\n",
    "# 이전 단계의 결과를 바탕으로 계획을 업데이트하거나 최종 응답을 반환\n",
    "def replan_step(state: PlanExecute):\n",
    "    output = replanner.invoke(state)\n",
    "    # 응답이 사용자에게 반환될 경우\n",
    "    if isinstance(output.action, Response):\n",
    "        return {\"response\": output.action.response}\n",
    "    # 추가 단계가 필요할 경우 계획의 단계 리스트 반환\n",
    "    else:\n",
    "        next_plan = output.action.steps\n",
    "        if len(next_plan) == 0:\n",
    "            return {\"response\": \"No more steps needed.\"}\n",
    "        else:\n",
    "            return {\"plan\": next_plan}\n",
    "\n",
    "\n",
    "# 에이전트의 실행 종료 여부를 결정하는 함수\n",
    "def should_end(state: PlanExecute):\n",
    "    if \"response\" in state and state[\"response\"]:\n",
    "        return \"final_report\"\n",
    "    else:\n",
    "        return \"execute\"\n",
    "\n",
    "\n",
    "final_report_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are given the objective and the previously done steps. Your task is to generate a final report in markdown format.\n",
    "Final report should be written in professional tone.\n",
    "\n",
    "Your objective was this:\n",
    "\n",
    "{input}\n",
    "\n",
    "Your previously done steps(question and answer pairs):\n",
    "\n",
    "{past_steps}\n",
    "\n",
    "Generate a final report in markdown format. Write your response in Korean.\"\"\"\n",
    ")\n",
    "\n",
    "final_report = (\n",
    "    final_report_prompt\n",
    "    | ChatGoogleGenerativeAI(model=MODEL_NAME, temperature=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "def generate_final_report(state: PlanExecute):\n",
    "    past_steps = \"\\n\\n\".join(\n",
    "        [\n",
    "            f\"Question: {past_step[0]}\\n\\nAnswer: {past_step[1]}\\n\\n####\"\n",
    "            for past_step in state[\"past_steps\"]\n",
    "        ]\n",
    "    )\n",
    "    response = final_report.invoke({\"input\": state[\"input\"], \"past_steps\": past_steps})\n",
    "    return {\"response\": response}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7ebe60",
   "metadata": {},
   "source": [
    "## 그래프 생성\n",
    "\n",
    "이제 지금까지 정의한 노드를 연결하여 그래프를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16abdcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "# 그래프 생성\n",
    "workflow = StateGraph(PlanExecute)\n",
    "\n",
    "# 노드 정의\n",
    "workflow.add_node(\"planner\", plan_step)\n",
    "workflow.add_node(\"execute\", execute_step)\n",
    "workflow.add_node(\"replan\", replan_step)\n",
    "workflow.add_node(\"final_report\", generate_final_report)\n",
    "\n",
    "# 엣지 정의\n",
    "workflow.add_edge(START, \"planner\")\n",
    "workflow.add_edge(\"planner\", \"execute\")\n",
    "workflow.add_edge(\"execute\", \"replan\")\n",
    "workflow.add_edge(\"final_report\", END)\n",
    "\n",
    "# 조건부 엣지: replan 후 종료 여부를 결정하는 함수 사용\n",
    "workflow.add_conditional_edges(\n",
    "    \"replan\",\n",
    "    should_end,\n",
    "    {\"execute\": \"execute\", \"final_report\": \"final_report\"},\n",
    ")\n",
    "\n",
    "# 그래프 컴파일\n",
    "app = workflow.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd850b8c",
   "metadata": {},
   "source": [
    "그래프를 시각화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ee7d379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "visualize_graph(app, xray=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d266cb",
   "metadata": {},
   "source": [
    "## 그래프 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6323963b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mplanner\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Naive RAG의 기본 개념과 구성 요소를 설명합니다.\n",
      "Modular RAG의 개념을 Naive RAG와 비교하여 설명하고, 추가되거나 변경된 모듈(예: 쿼리 변환, 라우팅, 재정렬)을 중심으로 상세히 설명합니다.\n",
      "Production level에서 Modular RAG를 사용하는 것의 이점(성능, 확장성, 유지보수성 등)을 구체적인 예시와 함께 설명합니다.\n",
      "두 RAG 방식의 차이점과 Modular RAG의 이점을 요약하여 최종 답변을 구성합니다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_web_search (19f00670-c6b0-4387-8cb4-380befcced29)\n",
      " Call ID: 19f00670-c6b0-4387-8cb4-380befcced29\n",
      "  Args:\n",
      "    query: Naive RAG 기본 개념\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mtools\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_web_search\n",
      "\n",
      "[{\"url\": \"https://day-to-day.tistory.com/76\", \"title\": \"Basic RAG를 실습 코드와 함께 알아보기 (feat. llamaIndex, langchain)\", \"content\": \"* langchain과 llamaIndex를 활용한 chromaDB로 Naive RAG 구현하기 특히 최신 정보를 답변하지 못하거나 **hallucination 문제**로 인해 구글, OpenAI, Facebook 등 여러 연구 기관과 기업들이 검색 증강 생성(Retrieval-Augmented Generation, RAG) 구조를 도입하게 되었다. LLM만 사용하는 것과 달리 RAG를 추가하면 특정 도메인이나 **조직 내부 데이터를 활용**함으로써 더욱 정밀한 답변을 생성할 수 있다. 또한 RAG를 사용하면 **출처를 기반으로 답변을 생성**하기 때문에 답변의 근거를 제시할 수 있다는 장점도 있다. **Naive RAG**는 가장 간단하고 기본적인 RAG이고, 오늘은 이 Naive RAG에 대해서 간단하게 살펴보고자 한다. **Naive RAG** Naive RAG의 구조를 보면 단순한 RAG 패러다임으로 주요 구성 요소는 **인덱싱(Indexing), 검색(Retrieve), 생성(Generation)**이 있다. Naive RAG를 보며 RAG가 동작하는 기본 개념을 먼저 이해하면 RAG 구조의 큰 틀을 확인하고 갈 수 있다. ### Naive RAG 위의 RAG 워크플로우를 간단하게 설명하면 원하는 문서를 **검색하기 좋은 형태로 vectordb에 한번에 모두 저장**한다.\", \"score\": 0.8763069, \"raw_content\": \"[본문 바로가기](#dkBody)\\n\\n# Day to\\\\_day\\n\\n[Day to\\\\_day](/)\\n\\n\\nm\\\\_inglet\\n\\n---\\n\\n## Day to\\\\_day\\n\\n## Basic RAG를 실습 코드와 함께 알아보기 (feat. llamaIndex, langchain) 본문\\n\\n**[LLM Project](/category/LLM%20Project)**\\n\\n### Basic RAG를 실습 코드와 함께 알아보기 (feat. llamaIndex, langchain)\\n\\nm\\\\_inglet\\n2024. 11. 10. 17:21\\n\\n728x90\\n\\n반응형\\n\\n### 들어가며\\n\\n* RAG를 사용해야 하는 이유\\n* RAG의 패러다임 (RAG 변천사)\\n* Naive RAG의 구조\\n* langchain과 llamaIndex를 활용한 chromaDB로 Naive RAG 구현하기\\n\\n### What is RAG(Retrieval-Augmented Generation)?\\n\\n전통적인 LLM들은 특정 시점의 데이터로만 훈련되어 훈련 이후의 정보는 반영하지 못한다. 특히 최신 정보를 답변하지 못하거나 **hallucination 문제**로 인해 구글, OpenAI, Facebook 등 여러 연구 기관과 기업들이 검색 증강 생성(Retrieval-Augmented Generation, RAG) 구조를 도입하게 되었다. 이에 따라 다른 여러 기업에서도 특히 환각 현상을 해결하기 위해 RAG에 대한 주목이 커지기 시작했다.\\n\\nLLM만 사용하는 것과 달리 RAG를 추가하면 특정 도메인이나 **조직 내부 데이터를 활용**함으로써 더욱 정밀한 답변을 생성할 수 있다. Finetuning으로 LLM에 직접 지식 주입을 할 수도 있겠지만 시간과 비용이 많이 들고 데이터가 업데이트됨에 따라 매번 finetuning을 진행하는 것은 한계가 있다. 또한 RAG를 사용하면 **출처를 기반으로 답변을 생성**하기 때문에 답변의 근거를 제시할 수 있다는 장점도 있다. 그리고 초기 LLM에 비해 점차 더 많은 token을 처리할 수 있게 되면서 context를 LLM에 넣어줄 수 있게되어 RAG를 활용하는 사례도 계속해서 발전해 나아가고 있다.\\n\\n### RAG 변천사\\n\\nRAG는 위의 같은 패러다임을 거쳐서 계속해서 발전하고 있다. **Naive RAG**는 가장 간단하고 기본적인 RAG이고, 오늘은 이 Naive RAG에 대해서 간단하게 살펴보고자 한다.\\n\\n그리고 이후 **Advanced RAG**는 pre-retrieval과 post-retrieval이 붙은 구조이다. 이는 더 정교한 검색을 할 수 있도록 더욱 발전된 형태의 RAG이다. 그리고 세번째 **Modular RAG**는 여러 모듈들로 이루어져 있고, 각각의 기능 별로 나누어져 있고, 이 여러 모듈을 레고처럼 조합해서 검색과 생성을 하는 RAG로 발전하게 되었다.\\n\\n**Naive RAG**\\n\\nNaive RAG의 구조를 보면 단순한 RAG 패러다임으로 주요 구성 요소는 **인덱싱(Indexing), 검색(Retrieve), 생성(Generation)**이 있다. Naive RAG를 보며 RAG가 동작하는 기본 개념을 먼저 이해하면 RAG 구조의 큰 틀을 확인하고 갈 수 있다. 세부적인 내용은 뒤에서 더 소개할 텐데 우선 이 방법은 검색의 정확도가 낮고, 그로 인한 낮은 품질의 응답을 생성한다는 한계가 있다.\\n\\n**Advanced RAG**\\n\\n이런 부족한 점을 개선하기 위해서 크게 Pre-Retrieval Process, Post-Retrieval Process을 추가하면서 더 다양한 기법을 통해 검색과 생성의 질을 크게 향상했다. Naive RAG에 비해서는 더 많이 개선되기는 했지만 애플리케이션에서의 사용 사례는 실제로는 더욱 복잡하기 때문에 다양한 케이스를 모두 커버하는데 한계가 존재했다. 즉, **한 방향의 RAG 흐름에서는 다양한 데이터를 모두 통합하는데에 파이프라인을 제어하기 어렵다**는 것이 한계점으로 지적되었다.\\n\\n**Modular RAG**\\n\\n그래서 Modular RAG는 여러 기능을 모두 모듈화를 시켜 재구성이 용이하고 유연한 흐름을 만들 수 있도록 고안된 방법이다. 특히 동적으로 레고처럼 구성할 수 있다는 것에서 다양한 플로우를 만들어 낼 수 있다.\\n\\n### Naive RAG\\n\\n이제 가장 기본이 되는 Naive RAG에 대해서 자세하게 들어가보자. \\n\\n위의 RAG 워크플로우를 간단하게 설명하면 원하는 문서를 **검색하기 좋은 형태로 vectordb에 한번에 모두 저장**한다. 이후에 유저가 보내는 **쿼리와** **가장 관련된** **콘텐츠들을 검색**, 그리고 그 콘텐츠와 유저 쿼리를 llm에 함께 넣어 답변을 생성하게 된다. 크게 보면 문서를 DB에 저장하는 단계, 유저가 사용할 때 일어나는 단계로 나눠볼 수 있다. 그 과정에 대해 자세히 알아보자.\\n\\n**1. 인덱싱(Indexing)**\\n\\n인덱싱은 데이터 소스(source)에서 문서를 파싱하여 데이터를 얻고 인덱스를 생성하는 과정으로 **데이터 정제**, **청크(chunk) 분할**, **벡터 인코딩 및 인덱스 생성**을 포함한다.\\n\\n**0-1. 데이터 로드(load data)**\\n\\n- 다양한 타입(web page, text, csv, pdf etc..)의 데이터를 로드하는 단계로 문서를 바로 읽어오기도, 혹은 파서를 이용해 전처리 작업을 거쳐 원하는 형태의 데이터로 변환하는 단계이다.\\n\\n- 예) pdf 파일을 읽어서 마크다운으로 변환, 캡쳐된 문서를 읽어와서 OCR로 텍스트로 변환\\n\\n**0-2. 텍스트 분할(chunk split)**\\n\\n- 불러온 데이터를 작은 크기의 단위(chunk)로 분할하는 작업이다. 텍스트가 분리되는 기준에 따라 ‘청크마다 독립적인 의미를 갖도록’ 나눌 수도 있고, 혹은 문장, 구절, 단락 등의 문장 구조를 기준으로 나눌 수 있다.\\n\\n- 청크 크기 역시 직접 조정하여 제한하거나 단어 수, 문자 수로 기준으로 나눌 수 있다.\\n\\n**0-3. 임베딩**\\n\\n- 위의 워크플로우에는 빠져있지만 우리는 이렇게 분할된 텍스트를 **검색 가능한 형태로 만드는 단계**로 만들어야한다.\\n\\n- 문자 자체로 비교하는 것보다 **언어를 벡터 형태**로 만들어서 유사도를 비교하는 게 더 쉽다.\\n\\n- 즉, 텍스트를 임베딩으로 변환하고, 이를 저장한 후 저장된 임베딩을 기반으로 유사성 검색을 수행하는 과정이다.\\n\\n- 이때 청크를 인덱싱하는 과정에서 window의 제한이 있고, 그 제한된 사이즈 안에서 해당 chunk를 벡터화시켜서 압축을 시켜 저장한다.\\n\\n**2. 검색 (Retrival)**\\n\\n- 청크를 벡터로 만들고 그것을 vectorDB에 저장했다면 저장된 벡터들 중에서 사용자의 쿼리(question)에 가장 유사한 벡터를 빠르게 찾아내는 작업이 필요하다.\\n\\n- 청크를 **벡터로 변환할 때 사용했던 임베딩 모델과 같은 모델을 사용**해 사용자의 쿼리도 임베딩 시킨다.\\n\\n- 이 검색하는 방법으로는 코사인 유사도, 유클리드 거리, 맨해튼 거리 등 다양한 **유사도 측정 방법을 사용** 가능하지만 주로 코사인 유사도를 텍스트 임베딩 검색에 자주 사용한다.\\n\\n- 사용자의 쿼리에 대해 계산된 **유사도 점수를 기반**으로 가장 유사한 항목들을 순서대로 **top k개**를 사용자에게 반환한다.\\n\\n**3. 생성 (Generation)**\\n\\n주어진 질문과 RAG를 통해 검색된 관련 context를 결합하여 새로운 프롬프트를 생성하고, llm은 이 정보를 기반으로 질문에 답변하는 과정이다.\\n\\n### 코드 구현하기\\n\\nLLM을 활용한 애플리케이션 개발을 간편하게 도와주는 도구로는 대표적으로 [langchain](https://python.langchain.com/docs/introduction/)과 [llamaIndex](https://docs.llamaindex.ai/en/stable/)가 있다. 현재 업무에서는 llamaIndex를 사용하고 있는데 개인적으로는 langchain이 좀 더 프로젝트 단위로 관리하기가 편한 것 같다. 물론 두 도구 모두 공식 문서가 잘 되어있기 때문에 참고해서 쉽게 구현이 가능한데 이 글을 읽게 되는 분들이 어떤 것을 사용할지 모르니 두 개의 예시 모두 적어두려 한다.\\n\\n코드는 문서를 Chroma DB에 저장하고, 쿼리를 보내보는 실습 코드를 가져왔다.\\n\\n#### langchain을 활용한 코드 예시\\n\\nlangchain== 0.3.7 버전으로 작성된 코드입니다. \\n\\n1. 문서 로드 후 splitter로 청킹, 모든 chunk를 ChromaDB에 저장하기 (경로를 지정해 주어 디스크에 저장)\\n\\n2. ChromaDB 로드해서 retriever로 만들기\\n\\n3. Naive RAG로 문서 검색 후 chain 구성해서 답변 생성\\n\\nchain 구조: retriever를 통해 검색된 chunk를 format\\\\_docs 함수에 넣어 content만 가져와서 이어 붙인다. \\n\\n그 내용은 모두 template에 context 자리에 들어가게 된다. \\n\\n입력 쿼리는 RunnablePassthrough를 통해서 그대로 template의 question 자리에 전달된다. \\n\\n이후 template은 llm의 입력으로 전달되고 output parser를 통해 response만 얻게 된다. \\n\\n#### \\n\\n#### llamaIndex를 활용한 코드 예시\\n\\nllama-index == 0.11.22 버전으로 작성된 코드입니다.\\n\\n1. ChromaDB에 문서를 저장\\n\\n문서를 가져와서 VectorStoreIndex의 from\\\\_documents로 청크를 분할하고 데이터 저장한다. \\n\\n2. query\\\\_engine을 이용해서 답변을 생성\\n\\nChromaDB를 불러와서 as\\\\_query\\\\_engine으로 query\\\\_engine을 생성한 뒤에 입력 쿼리 전달한다. \\n\\nquery\\\\_engine을 사용하지 않고 OpenAI API를 직접 호출해서 사용하는 방법도 있는데 그것은 github으로 업로드해 두겠다. \\n\\n단계별로 자세한 코드는 아래 github을 통해서 확인할 수 있습니다. \\n\\n### \\n\\n### Reference\\n\\n[RAG from scratch](https://www.youtube.com/watch?v=wd7TZ4w1mSw&list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x)\\n\\n[대규모 언어 모델을 위한 검색-증강 생성(RAG) 기술 현황 - 1/2편](https://discuss.pytorch.kr/t/rag-1-2/3135)\\n\\n<https://wikidocs.net/231431>\\n\\n<https://python.langchain.com/docs/tutorials/rag/>\\n\\n728x90\\n\\n반응형\\n\\n\\n\\n\\n[저작자표시 비영리 변경금지\\n(새창열림)](https://creativecommons.org/licenses/by-nc-nd/4.0/deed.ko)\\n\\n#### '[LLM Project](/category/LLM%20Project)' 카테고리의 다른 글\\n\\n|  |  |\\n| --- | --- |\\n| [Agent의 개념과 주요 구성 요소에 대해 파악하기](/82)  (0) | 2025.03.30 |\\n| [나만의 프롬프트 엔지니어링 기법을 하나씩 공유해보자 (LLM-as-a-Judge, Agent)](/81)  (0) | 2025.02.16 |\\n\\n**'LLM Project' Related Articles**\\n\\n* [Agent의 개념과 주요 구성 요소에 대해 파악하기\\n  2025.03.30](/82?category=1186091)\\n* [나만의 프롬프트 엔지니어링 기법을 하나씩 공유해보자 (LLM-as-a-Judge, Agent)\\n  2025.02.16](/81?category=1186091)\\n\\n[more](/category/LLM%20Project)\\n\\n[Day to\\\\_day](https://day-to-day.tistory.com)[어찌저찌 하긴 함](https://day-to-day.tistory.com)\\n\\n댓글2\\n\\n* [쓰소미](https://handclick.tistory.com)\\n\\n  포스팅 잘 보았어요✨\\n\\n  2024. 11. 10. 20:53[답글](#)\\n\\n  + [신고](#)\\n  + [링크복사](#)\\n* 익명비밀댓글\\n\\n  비밀댓글입니다.\\n\\n  2024. 11. 10. 21:44\\n\\n  + [링크복사](#)\\n\\n---\\n\\nBlog is powered by\\n[kakao](http://www.kakaocorp.com) / Designed by\\n[Tistory](http://www.tistory.com)\\n\\n\\n\\n\\n\\n## 단축키\\n\\n### 내 블로그\\n\\n|  |  |\\n| --- | --- |\\n| 내 블로그 - 관리자 홈 전환 | `Q` |\\n| 새 글 쓰기 | `W` |\\n\\n### 블로그 게시글\\n\\n|  |  |\\n| --- | --- |\\n| 글 수정 (권한 있는 경우) | `E` |\\n| 댓글 영역으로 이동 | `C` |\\n\\n### 모든 영역\\n\\n|  |  |\\n| --- | --- |\\n| 이 페이지의 URL 복사 | `S` |\\n| 맨 위로 이동 | `T` |\\n| 티스토리 홈 이동 | `H` |\\n| 단축키 안내 | `Shift` + `/`  `⇧` + `/` |\\n\\n\\\\* 단축키는 한글/영문 대소문자로 이용 가능하며, 티스토리 기본 도메인에서만 동작합니다.\"}, {\"url\": \"https://baruninfo.co.kr/52\", \"title\": \"RAG란? (RAG 개념과 활용) - 트렌드를 쫓는 AI엔지니어\", \"content\": \"RAG(Retrieval-Augmented Generation)는 LLM이 외부 지식 베이스나 문서 저장소에서 질문과 관련된 정보를 검색하고, 이를 프롬프트에 포함시켜 더 정확하고 신뢰성 있는 답변을 생성하는 기술입니다. 학습 데이터셋을 추가하는 대신, 도메인별 특화 데이터베이스와 기존 학습 데이터를 통합하는 방식을 채택했는데, 이것이 바로 RAG(Retrieval-Augmented Generation, 검색 증강 생성)입니다. RAG(Retrieval-Augmented Generation)의 발전 과정에서 Naive RAG, Advanced RAG, Modular RAG는 각기 다른 수준의 복잡성과 기술적 정교함을 갖춘 접근 방식입니다. 이는 인덱싱(Indexing), 검색(Retrieval), 생성(Generation)이라는 RAG의 기본 프로세스를 충실히 따르며, 간단히 '검색과 읽기'로 요약할 수 있습니다. 생성된 응답은 사용자가 찾고자 한 정보를 명확하고 간결하게 전달합니다.이러한 질문에서 응답까지의 데이터 흐름을 통해 Naive RAG 시스템의 각 구성 요소가 정확하고 관련성 높은 정보를 제공하는 데 중요한 역할을 함을 알 수 있습니다. * 쿼리를 검색 전에 재작성하거나 확장함으로써, Advanced RAG는 사용자의 의도를 정확히 파악하여 더 적절하고 관련성 높은 결과를 제공합니다.\", \"score\": 0.8018276, \"raw_content\": \"# [트렌드를 쫓는 AI엔지니어](https://baruninfo.co.kr/)\\n\\nRAG(Retrieval-Augmented Generation)는 LLM이 외부 지식 베이스나 문서 저장소에서 질문과 관련된 정보를 검색하고, 이를 프롬프트에 포함시켜 더 정확하고 신뢰성 있는 답변을 생성하는 기술입니다. 이 기술은 LLM의 학습 데이터에 포함되지 않은 최신 정보나 도메인 특화 지식을 활용할 수 있게 해주며, 할루시네이션(Hallucination) 현상을 줄이는 데 도움을 줍니다.\\n\\n---\\n\\n## RAG의 개념과 특징\\n\\n### 1. RAG 개념이 생기게된 이유\\n\\nChatGPT-3 모델이 큰 관심을 받으면서 사용자들이 다양한 결과물을 공유하던 시기가 있었습니다. 그중 \\\"세종대왕 맥북프로 던짐 사건\\\"[1]은 한동안 인터넷에서 화제가 된 밈이었죠. (세종대왕 맥북 던짐 사건 나무위키)\\n\\n이 사건을 계기로 GPT와 같은 AI의 할루시네이션(잘못된 정보나 허구의 내용을 생성하는 문제)이 치명적인 단점으로 인식되었습니다.\\n\\n흥미로운 점은 LLM 연구자들이 이미 이러한 거짓 정보 문제를 인지하고 있었다는 것입니다. 다만 '할루시네이션'이라는 용어 대신 '정확성 부족'이라고 표현했을 뿐이죠. 이 문제를 해결하기 위해 주로 Fine-Tuning 기법을 사용했지만, 대규모 모델의 재학습에 따른 번거로움과 느린 업데이트 속도가 한계로 작용했습니다.\\n\\nsLLM, Fine-Tuning, 프롬프트 엔지니어링 등 LLM 모델의 할루시네이션을 해결하기 위한 다양한 접근법이 연구되었습니다. 하지만 기존 신경망에 특정 도메인 데이터를 추가로 학습시키고 모델을 fine-tuning하는 과정에서 높은 계산 비용이 발생해 쉽게 해결책을 찾지 못했습니다.\\n\\n이러한 문제를 해결하기 위해 새로운 접근법이 도입되었습니다. 학습 데이터셋을 추가하는 대신, 도메인별 특화 데이터베이스와 기존 학습 데이터를 통합하는 방식을 채택했는데, 이것이 바로 RAG(Retrieval-Augmented Generation, 검색 증강 생성)입니다.\\n\\n### 2. RAG 정의와 발전\\n\\nRAG는 LLM이 외부 지식 베이스에서 관련 정보를 검색하고 이를 활용하여 더욱 정확하고 신뢰성 있는 답변을 생성하는 기술입니다.\\n\\n초창기 RAG는 사전 학습된 검색기(Retriever)와 생성기(seq2seq 모델)를 결합하여 지식을 해석 가능하고 모듈화된 방식으로 처리했습니다.[2] 검색기로 질문에 대한 관련 정보를 찾고, 이를 바탕으로 답변을 생성하는 방식이었죠.\\n\\nChatGPT 출시 이후 RAG 연구가 급격히 증가했습니다. RAG는 거대 모델의 재학습이나 fine-tuning 없이도 비용 효율적으로 모델의 생성 능력을 강화할 수 있는 기술이었기 때문입니다.\\n\\n2022년부터 RAG 연구는 크게 3가지 패러다임으로 발전했습니다. 기본적인 검색과 생성을 통합한 Naive RAG, 검색 과정을 고도화하여 검색 품질에 집중한 Advanced RAG, 그리고 전통적인 검색 방법에 전처리와 후처리 과정을 모듈화하여 추가한 Modular RAG가 대표적입니다. 이러한 세 가지 RAG 유형에 대해서는 뒤에서 자세히 설명하겠습니다.\\n\\n### 3. RAG의 핵심 특징\\n\\nRAG는 쉽게 이해할 수 있는 개념입니다. 사람이 정답을 찾는 과정과 비슷하기 때문인데요. 우리는 보통 궁금한 문제에 대한 개념을 먼저 찾고, 그 개념을 이해하고 정리하면서 정답에 도달합니다.\\n\\n위 그림처럼 서울역에 가는 가장 빠른 경로를 찾을 때, 우리는 보통 지도 앱을 실행하여 현재 위치에서 서울역까지의 최단 시간 경로를 검색합니다.\\n\\n만약 GPT와 같은 LLM으로 이 답을 찾으려면, LLM은 질문자의 현재 위치부터 목적지까지의 모든 경로와 시간 정보를 사전에 학습해야 합니다. 하지만 전 세계의 위치 정보와 각국의 대중교통, 소요 시간 등을 정확하게 학습하는 것은 현실적으로 불가능합니다.\\n\\n이런 맥락에서 RAG를 적용하면, 지도 앱이 검색기 역할을 하고 그 결과를 기반으로 LLM이 답변을 생성하게 됩니다. 즉, LLM이 모든 정보를 추가로 학습할 필요 없이 외부 지식을 입력받아 참고하면서 답변을 만들 수 있습니다. 이러한 RAG 방식은 답변의 정확도와 관련성을 높이고, LLM의 할루시네이션 문제를 줄이며, 전반적인 신뢰성을 향상시킬 수 있습니다.\\n\\n---\\n\\n## RAG의 3가지 패러다임\\n\\nRAG(Retrieval-Augmented Generation)의 발전 과정에서 Naive RAG, Advanced RAG, Modular RAG는 각기 다른 수준의 복잡성과 기술적 정교함을 갖춘 접근 방식입니다.\\n\\n### 1. Naive RAG\\n\\nNaive RAG는 RAG의 가장 기본적인 개념을 구현한 방식입니다. 이는 인덱싱(Indexing), 검색(Retrieval), 생성(Generation)이라는 RAG의 기본 프로세스를 충실히 따르며, 간단히 '검색과 읽기'로 요약할 수 있습니다.\\n\\n이는 기본적인 검색 시스템과 유사하게 작동합니다. 외부 지식베이스의 내용을 토큰 임베딩으로 데이터베이스화한 뒤, 사용자 질문과 관련된 지식을 데이터베이스에서 검색하여 LLM에 전달하고 답변을 생성하는 방식입니다.\\n\\n#### Naive RAG 핵심 컴포넌트\\n\\n* Document Chunking(문서 조각화)\\n\\n외부 지식베이스를 만들기 위해서는 문서를 더 작은 조각으로 나누는 것부터 시작합니다. 문서를 그대로 입력하는 대신, 관리와 검색이 용이하도록 세분화합니다. 예를 들어, 여러 문서를 하나로 통합한 문서나 긴 문서의 경우 세그먼트 단위로 나누어 정보 검색 성능을 최적화합니다.\\n\\n* Embedding Model(임베딩모델)\\n\\n임베딩 모델은 RAG의 핵심 요소입니다. 외부 지식을 담은 문서 조각과 사용자의 질문을 수학적으로 표현하여 텍스트를 벡터화하는 모델입니다.\\n\\n* Vector Database\\n\\n문서의 임베딩 결과값은 벡터 데이터베이스에 저장됩니다. 벡터 데이터베이스는 임베딩을 효율적으로 저장하고 검색할 수 있도록 설계되었습니다. 사용자의 질문을 쿼리로 변환하여 데이터베이스의 임베딩과 비교함으로써 가장 관련성 높은 문서 조각을 찾아냅니다.\\n\\n* Retrieval(검색)\\n\\n벡터 데이터베이스가 관련 문서 조각을 식별하면 관련성이 높은 문서를 검색합니다. 이 검색 과정은 최종 응답 생성에 사용할 정보를 선별하는 필터 역할을 합니다.\\n\\n* LLM\\n\\nLLM은 검색된 관련 문서를 처리하여 사용자의 질문에 대한 일관성 있고 적절한 응답을 생성합니다. 사용자의 질문과 검색된 문서 조각을 활용하여 맥락적으로 적합한 응답을 제공하며, 데이터를 해석하여 이해하기 쉬운 자연어로 응답을 구성합니다.\\n\\n* Response Generation(응답 생성)\\n\\n마지막 단계에서 시스템은 LLM이 처리한 정보를 바탕으로 응답을 생성합니다. 생성된 응답은 사용자가 찾고자 한 정보를 명확하고 간결하게 전달합니다.이러한 질문에서 응답까지의 데이터 흐름을 통해 Naive RAG 시스템의 각 구성 요소가 정확하고 관련성 높은 정보를 제공하는 데 중요한 역할을 함을 알 수 있습니다.\\n\\n---\\n\\n### 2. Advaned RAG\\n\\n고급 RAG(Advanced RAG)는 Naive RAG를 기반으로 프로세스를 한층 더 정교화한 시스템입니다. Naive RAG가 검색된 정보를 단순히 응답에 통합하는 것과 달리, Advanced RAG는 응답의 관련성과 전반적인 품질을 최적화하기 위한 추가 처리 단계들을 포함합니다.\\n\\n#### Advanced RAG 핵심 컴포넌트\\n\\n* Query Optimization (Query Rewriting - Pre-Retrieval Process)\\n\\nNaive RAG는 사용자의 질문을 바로 임베딩해 벡터 데이터베이스에서 질의하는 것과 달리, Advanced RAG에서는 사용자의 질문을 한 번 더 최적화하는 단계가 포함됩니다.\\n\\n사용자의 질의를 분석하고 전처리하여 검색 정확도를 향상시키는 것을 목표로 하며, 쿼리 확장(Expansion)이나 쿼리 재구성을 통해 사용자의 의도를 더 정확하게 반영하도록 수정합니다.\\n\\n* Dynamic Embeddings (동적 임베딩)\\n\\nNaive RAG는 대체로 모든 데이터 유형에 단일 임베딩 모델을 사용합니다. 반면 Advanced RAG는 특정 작업이나 도메인에 맞춰 임베딩을 fine-tuning하고 조정합니다. 이를 통해 임베딩 모델이 특정 유형의 쿼리나 데이터셋에 필요한 맥락적 이해(특징점)를 더 정확하게 포착할 수 있게 됩니다.\\n\\n* Hybrid Search (하이브리드 검색)\\n\\n하이브리드 검색은 쿼리의 특성과 관계없이 높은 관련성을 가진 정보를 검색할 수 있도록 보장하는 기법입니다.\\n\\nAdvanced RAG는 벡터 데이터베이스의 검색 성능을 향상시키기 위해 여러 검색 전략을 결합합니다. 키워드 기반 검색, 의미 검색, 신경망 검색 등 다양한 검색 기법을 통합적으로 활용하는 것이 특징입니다.\\n\\n* Post-Retreval Processing (검색 후 처리)\\n\\n벡터 데이터베이스에서 검색한 이후에도 검색된 데이터를 추가로 처리하여 최종 출력물의 검색 결과 품질과 관련성을 높입니다.\\n\\n검색 후 처리에는 Re-Ranking(재정렬)이 포함되며, 이 과정을 통해 사용자 질문에 가장 관련성과 유용성이 높은 데이터에 우선순위를 매깁니다.\\n\\n* Re-Ranking (재정렬)\\n\\nAdvanced RAG는 검색 과정 이후에 정보를 더욱 정제하는 추가 단계를 수행합니다. 사용자의 질문을 중심으로 검색된 정보의 관련성과 맥락성을 평가하여 우선순위를 부여합니다.\\n\\n이 과정에서 검색된 결과를 쿼리와의 의미적 관련성과 맥락 적합성을 기준으로 재평가하고 정렬합니다. 가장 관련성이 높은 정보를 상위에 배치함으로써 응답의 정확성과 일관성을 보장하는 것을 목표로 합니다.\\n\\n* Context Compression (맥락 압축)\\n\\n필터링과 재정렬 알고리즘을 사용한 후에도 사용자 쿼리에 답변하는 데 불필요한 데이터가 남아있을 수 있습니다. 이러한 불필요한 데이터를 제거하는 과정을 맥락 압축(Context Compression)이라고 합니다.\\n\\n이 단계는 관련 문서를 LLM에 전달하기 직전에 적용되어, LLM이 가장 관련성 높은 정보만을 받도록 합니다. 이를 통해 LLM은 최상의 결과를 생성할 수 있습니다.\\n\\n---\\n\\n#### Advanced RAG의 특징 정리\\n\\n**재정렬을 통한 더 나은 관련성**\\n\\n* 재정렬(Re-ranking)은 가장 관련성 높은 정보를 우선순위로 배치해 최종 응답의 정확성과 논리적 흐름을 향상시킵니다.\\n\\n**맥락을 더 잘 반영하는 동적 임베딩**\\n\\n* 동적 임베딩(Dynamic Embeddings)은 특정 작업에 맞춰 최적화되어, 시스템이 다양한 쿼리를 더 정확하게 이해하고 응답할 수 있게 합니다.\\n\\n**하이브리드 검색을 통한 더 정확한 검색**\\n\\n* 하이브리드 검색(Hybrid Search)은 여러 검색 전략을 통합해 데이터를 더 효과적으로 찾아내고, 결과의 관련성과 정확도를 높입니다.\\n\\n**맥락 압축을 통한 효율적인 응답**\\n\\n* 맥락 압축(Context Compression)은 불필요한 정보를 제거하여 처리 속도를 향상시키고, 더욱 명확하고 고품질의 답변을 제공합니다.\\n\\n**사용자 쿼리 이해도 향상**\\n\\n* 쿼리를 검색 전에 재작성하거나 확장함으로써, Advanced RAG는 사용자의 의도를 정확히 파악하여 더 적절하고 관련성 높은 결과를 제공합니다.\\n\\n---\\n\\n### 3. Modular RAG\\n\\n모듈 RAG는 Naive, Advanced보다 훨씬 더 복잡한 개념의 RAG입니다. 이번 RAG의 개념과 활용 포스팅에서는 간단히 정리만하고 자세한 내용은 별도의 포스팅으로 정리해보겠습니다.\\n\\n모듈 RAG의 핵심은 말 그대로 \\\"모듈(module)\\\"입니다. 모듈의 구조와 작동 방식에 따라 그 특성이 달라집니다. 각 모듈은 RAG 프로세스에서 특정 작업을 처리하고, 시스템 전체의 기능을 유지하는 데 중요한 역할을 합니다.\\n\\n#### Modular RAG 핵심 컴포넌트\\n\\n#### 1. 인덱싱 모듈 (Indexing Module)\\n\\n인덱싱 모듈은 RAG 시스템이 검색할 방대한 데이터를 조직화하고 구조화하는 역할을 합니다.\\n\\n* **주요 작업**\\n  + 문서를 관리 가능한 크기로 나눔.\\n  + 임베딩을 생성하고 이를 효율적으로 검색할 수 있도록 저장.\\n* **모듈의 목적**\\n  + 콘텐츠를 정확하게 표현하여 시스템이 쿼리와 가장 관련성 높은 조각을 효과적으로 매칭할 수 있도록 보장.\\n\\n#### 2. 검색 모듈 (Retrieval Module)\\n\\n검색 모듈은 사용자의 쿼리를 기반으로 가장 관련성 높은 데이터 조각을 선택하는 핵심 역할을 수행합니다.\\n\\n* **주요 작업**\\n  + 쿼리 확장(Query Expansion) 수행.\\n  + 검색된 조각을 재정렬(Reranking)하고 불필요한 데이터를 필터링.\\n* **모듈의 목적**\\n  + 검색 프로세스를 정교하게 개선하여, 후속 생성 단계에서 가장 유용하고 맥락적으로 적합한 정보를 반환.\\n\\n#### \\n\\n#### 3. 생성 모듈 (Generation Module)\\n\\n생성 모듈은 관련 데이터가 검색된 후 응답 생성 작업을 수행합니다.\\n\\n* **주요 작업**\\n  + 검색된 데이터를 활용하여 최종 출력(예: 쿼리 응답, 요약 등)을 생성.\\n  + **서브 모듈**: 출력의 정확성, 일관성, 사용자 요구에 맞춘 최적화를 수행.\\n* **모듈의 목적**\\n  + 사용자의 요구에 부합하는 정확하고 일관된 응답을 생성.\\n\\n#### 4. 오케스트레이션 모듈 (**Orchestration Module)**\\n\\nModular RAG에서 가장 혁신적인 요소 중 하나로, 각 모듈 간의 조정 및 관리를 담당합니다.\\n\\n* **주요 역할**:\\n  + 시스템 내 데이터와 작업의 흐름을 제어.\\n  + 쿼리의 특정 요구 사항에 따라 어떤 모듈을 활성화할지 결정.\\n* **모듈의 목적**\\n  + 프로세스를 조율함으로써 시스템이 다양한 시나리오에 동적으로 적응할 수 있도록 지원.\\n  + 시스템의 유연성과 효율성을 극대화.\\n\\n---\\n\\n## RAG모델 평가 방법\\n\\nRAG의 목적은 LLM의 단점을 해결하기 위한 것입니다. 그렇기 때문에 RAG 모델들의 성능 지표도 어느정도 정형화 되어있습니다.\\n\\n평가지표\\n\\n* 응답의 정확도\\n* 할루시네이션(환각) 감소 수준\\n* 투명성과 출처 인용 품질\\n\\n---\\n\\n## 맺음말\\n\\nRAG의 기본 개념부터 패러다임, 평가방법까지 프로세스 중심으로 정리했습니다.\\n\\nRAG는 LLM의 주요 단점인 할루시네이션 문제를 해결하기 위해 개발된 기술입니다. 외부 지식을 활용하여 더 정확하고 신뢰성 있는 답변을 생성할 수 있게 되었죠. RAG는 외부 지식을 생성 능력에 동적으로 통합함으로써 LLM의 한계를 극복하는 혁신적인 접근 방식입니다.\\n\\nRAG는 Naive, Advanced, Modular 등 다양한 형태로 발전했으며, 궁극적인 목적은 \\\"정확성\\\"과 \\\"효율성\\\"에 있습니다. 기업이나 서비스에서 RAG에 주목하는 이유가 명확합니다. 자체 데이터를 외부에 노출하지 않고, 고비용 학습 없이도 LLM의 장점을 극대화할 수 있기 때문이죠.\\n\\n2024년부터 국내에서도 RAG를 적극적으로 도입하기 시작했습니다. RAG의 개념을 이해하면서 기술적인 경험을 쌓아보는 건 어떨까요?\\n\\n참고 문헌\\n\\n[1] <https://namu.wiki/w/세종대왕%20맥북프로%20던짐%20사건?uuid=8f18b777-9009-4e52-a170-755574a38a1f>\\n\\n[2] [Retrieval-augmented generation for knowledge-intensive nlp tasks, Lewis et al., 2020](https://arxiv.org/pdf/2312.10997v1)\\n\\n[3] [Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/pdf/2312.10997v1)\\n\\n[4] <https://myscale.com/blog/naive-rag-vs-advanced-rag/>\\n\\n반응형\\n\\n\\n\\n[http://www.darwinai.ai](https://ader.naver.com/v1/e7r23tm1rzzrs5lf-d_70as_Z5U5-TTU1DMhZubIIZKJstK7BSFUZc9vz9qFU7z-BJkb2KCG2IXOAwpeDd6C1Zx1bEm1xEODvh64gPAehPGUGU5L48V9zqRRUXWMHvwnoU5Gzc7lmV9rMiX7PaWt-cJPmY5MZlDK1M3BAxL0zYUpAohhzHKyJJxTaSnILvZFxIEJXfPuniE3zueiLZAFp481y-9STDE-DD3hdqIVC-7L2GRWU5_v1a_vtHseAqXQbJ8e-N3ugO3yA0Y5tRNm4PjRMmXwaAP1Q5cTzqPfWluyL50VZWGkCrIafw7blVlPxWcJDGpqbl1APeL02hdDySZPKoxNRtvDhvQLlC3tVkajgo6tJ2iGN637up4_O1M8?c=tistory.ch1&t=0)광고\\n\\n[폐쇄망 오프라인 GPT챗봇](https://ader.naver.com/v1/e7r23tm1rzzrs5lf-d_70as_Z5U5-TTU1DMhZubIIZKJstK7BSFUZc9vz9qFU7z-BJkb2KCG2IXOAwpeDd6C1Zx1bEm1xEODvh64gPAehPGUGU5L48V9zqRRUXWMHvwnoU5Gzc7lmV9rMiX7PaWt-cJPmY5MZlDK1M3BAxL0zYUpAohhzHKyJJxTaSnILvZFxIEJXfPuniE3zueiLZAFp481y-9STDE-DD3hdqIVC-7L2GRWU5_v1a_vtHseAqXQbJ8e-N3ugO3yA0Y5tRNm4PjRMmXwaAP1Q5cTzqPfWluyL50VZWGkCrIafw7blVlPxWcJDGpqbl1APeL02hdDySZPKoxNRtvDhvQLlC3tVkajgo6tJ2iGN637up4_O1M8?c=tistory.ch1&t=0)[문서파일 업로드로 실시간 학습되는 폐쇄망 RAG 기반의 GPT!](https://ader.naver.com/v1/e7r23tm1rzzrs5lf-d_70as_Z5U5-TTU1DMhZubIIZKJstK7BSFUZc9vz9qFU7z-BJkb2KCG2IXOAwpeDd6C1Zx1bEm1xEODvh64gPAehPGUGU5L48V9zqRRUXWMHvwnoU5Gzc7lmV9rMiX7PaWt-cJPmY5MZlDK1M3BAxL0zYUpAohhzHKyJJxTaSnILvZFxIEJXfPuniE3zueiLZAFp481y-9STDE-DD3hdqIVC-7L2GRWU5_v1a_vtHseAqXQbJ8e-N3ugO3yA0Y5tRNm4PjRMmXwaAP1Q5cTzqPfWluyL50VZWGkCrIafw7blVlPxWcJDGpqbl1APeL02hdDySZPKoxNRtvDhvQLlC3tVkajgo6tJ2iGN637up4_O1M8?c=tistory.ch1&t=0)\\n\\n[https://cywellsystem.com/](https://ader.naver.com/v1/raf9EiIj6IbRpshUpWkGesiQcWzWlvnBmCsacYP2BqABaeuSeckLnyUjwBIySt5TAXqPqdiRTzI_qTKTCydfLy3UpAbYrRSISWLHuWnlT2OcVsh-buG5GN0iCXt5io00vN_vjrQTNSzUj-0xzOR30TGVA0x7L0M-nA28_Kqadamb5bPGAT4XZqMdmYEqiPOUzj3zI9uf79LC0mDRjUGtVT4HWlyh8fnJAVDjGLpZE0abIe7zNXUM9gKinkeJLNtY?c=tistory.ch1&t=0)광고\\n\\n[문서 AI는 RAG으로!](https://ader.naver.com/v1/raf9EiIj6IbRpshUpWkGesiQcWzWlvnBmCsacYP2BqABaeuSeckLnyUjwBIySt5TAXqPqdiRTzI_qTKTCydfLy3UpAbYrRSISWLHuWnlT2OcVsh-buG5GN0iCXt5io00vN_vjrQTNSzUj-0xzOR30TGVA0x7L0M-nA28_Kqadamb5bPGAT4XZqMdmYEqiPOUzj3zI9uf79LC0mDRjUGtVT4HWlyh8fnJAVDjGLpZE0abIe7zNXUM9gKinkeJLNtY?c=tistory.ch1&t=0)[쌓여있는 문서 활용성 향상 및 법 전문 ChatGPT연동으로 더 쉬운 내용 검색!](https://ader.naver.com/v1/raf9EiIj6IbRpshUpWkGesiQcWzWlvnBmCsacYP2BqABaeuSeckLnyUjwBIySt5TAXqPqdiRTzI_qTKTCydfLy3UpAbYrRSISWLHuWnlT2OcVsh-buG5GN0iCXt5io00vN_vjrQTNSzUj-0xzOR30TGVA0x7L0M-nA28_Kqadamb5bPGAT4XZqMdmYEqiPOUzj3zI9uf79LC0mDRjUGtVT4HWlyh8fnJAVDjGLpZE0abIe7zNXUM9gKinkeJLNtY?c=tistory.ch1&t=0)\\n\\n[저작자표시 비영리 변경금지\\n(새창열림)](https://creativecommons.org/licenses/by-nc-nd/4.0/deed.ko)\\n\\n## 태그\\n\\n[advancedrag](/tag/advancedrag), [moularrag](/tag/moularrag), [naiverag](/tag/naiverag), [Rag](/tag/Rag), [rag개념](/tag/rag%EA%B0%9C%EB%85%90)\\n\\n## 관련글\\n\\n* [RAG 검색 방법 비교(희소 검색, 밀집검색-Sparse Retriever vs Dense Retriever)](/55?category=1479863)\\n* [랭체인(Langchain) 소개 및 특징 (RAG필수 프레임워크)](/54?category=1479863)\\n* [어텐션(Attention) 이해하기](/51?category=1479863)\\n* [LLM이란 무엇인가?](/48?category=1479863)\\n\\n[트렌드를 쫓는 AI엔지니어](https://baruninfo.co.kr)[자연어처리와 LLM관련 연구 동향과 AI 관련 도움이되는 정보를 공유합니다.](https://baruninfo.co.kr)\\n\\n댓글2\\n\\n* [zemorae](https://review.jinbest.com)\\n\\n  정성스런 포스팅 잘 봤습니다^^함께 소통 하면 좋을거 같아요^^오늘도 건강한 하루 보내세요^^\\n\\n  2025. 1. 17. 12:50[답글](#)\\n\\n  + [신고](#)\\n  + [링크복사](#)\\n* [잡소](https://economicalprofit.tistory.com)\\n\\n  오늘도 넉넉하게 머물다 갑니다! 행복한 오늘 되세요+1꾸욱 하고 갑니다🫡\\n\\n  2025. 1. 17. 23:42[답글](#)\\n\\n  + [신고](#)\\n  + [링크복사](#)\\n\\n---\\n\\n\\n\\n\\n\\n\\n\\n## 단축키\\n\\n### 내 블로그\\n\\n|  |  |\\n| --- | --- |\\n| 내 블로그 - 관리자 홈 전환 | `Q` |\\n| 새 글 쓰기 | `W` |\\n\\n### 블로그 게시글\\n\\n|  |  |\\n| --- | --- |\\n| 글 수정 (권한 있는 경우) | `E` |\\n| 댓글 영역으로 이동 | `C` |\\n\\n### 모든 영역\\n\\n|  |  |\\n| --- | --- |\\n| 이 페이지의 URL 복사 | `S` |\\n| 맨 위로 이동 | `T` |\\n| 티스토리 홈 이동 | `H` |\\n| 단축키 안내 | `Shift` + `/`  `⇧` + `/` |\\n\\n\\\\* 단축키는 한글/영문 대소문자로 이용 가능하며, 티스토리 기본 도메인에서만 동작합니다.\"}, {\"url\": \"https://news.hada.io/topic?id=12669\", \"title\": \"대규모 언어 모델을 위한 검색-증강 생성(RAG) 기술 현황 - GeekNews\", \"content\": \"# 대규모 언어 모델을 위한 검색-증강 생성(RAG) 기술 현황 (discuss.pytorch.kr) RAG 외에 파인튜닝(fine-tuning)을 사용하여 대규모 언어 모델이 특정 지식에 집중하도록 할 수 있습니다. 주요 패러다임들은 기본 RAG, 고급 RAG, 모듈형 RAG의 3가지로, 초창기의 기본 RAG는 비용 효율적이고 LLM만 사용하는 것보다 성능이 뛰어났지만 많은 단점들이 있었습니다. ### RAG의 3가지 주요 패러다임 - 기본 RAG(Naive RAG) 기본 RAG는 RAG 연구의 초기 방법론을 지칭하며, 전통적인 인덱싱, 검색 및 생성 과정을 포함하고 있습니다. ### RAG의 3가지 주요 패러다임 - 고급 RAG(Advanced RAG) 고급 RAG는 크게 검색 전 절차(Pre-Retrieval Process), 검색 후 절차(Post-Retrieval Process), RAG 파이프라인 최적화(RAG Pipeline Optimization)의 3단계로 나누어볼 수 있습니다: ### RAG의 3가지 주요 패러다임 - 모듈형 RAG(Modular RAG) ### RAG 기술의 주요 구성 요소: 검색기(Retriever) ### RAG 기술의 주요 구성 요소: 증강 방법(Augmentation Methods)\", \"score\": 0.72395444, \"raw_content\": \" \\n\\n[▲](javascript:vote(12669, \\\"up\\\");)\\n\\n[# 대규모 언어 모델을 위한 검색-증강 생성(RAG) 기술 현황](https://discuss.pytorch.kr/t/rag-1-2/3135?utm_source=geeknews) (discuss.pytorch.kr)\\n\\n16P by [ninebow](/user?id=ninebow) 2024-01-03 | [★ favorite](javascript:fav(12669, \\\"favorite\\\")) | [댓글 2개](topic?id=12669)\\n\\n#### PyTorchKR\\n\\n* LLM의 인기와 함께 RAG에 대한 관심도 높아지고 있습니다. 파이토치 한국 사용자 모임에서는 [지난 12/18~24의 주요 ML 논문들](https://discuss.pytorch.kr/t/2023-12-18-12-24-ml-top-ml-papers-of-the-week/3111?utm_source=geeknews)에 소개된 [RAG 기술에 대한 서베이 논문](https://discuss.pytorch.kr/t/2023-12-18-12-24-ml-top-ml-papers-of-the-week/3111#retrieval-augmented-generation-for-large-language-models-a-survey-41)을 정리해보았습니다.\\n* 내용이 다소 길어 2개의 글로 나눠 게시하였으며, [1부에서는 RAG 기술의 패러다임들](https://discuss.pytorch.kr/t/rag-1-2/3135?utm_source=geeknews)을, [2부에서는 주요 구성요소들과 평가, 앞으로의 연구 과제들](https://discuss.pytorch.kr/t/rag-2-2/3160?utm_source=geeknews)에 대해서 정리했습니다.\\n\\n*아래는 목차 중심으로 주요 내용들만 발췌한 것입니다*\\n\\n---\\n\\n### 검색-증강 생성(RAG) 기술 소개\\n\\n대규모 언어 모델(LLM)은 뛰어난 능력을 보이지만, 실제 사용을 위해서는 환각이나 느린 지식의 업데이트, 답변의 투명성 부족 등과 같은 문제들을 극복해야 합니다. RAG(Retrieval-Augmented Generation)는 이러한 문제를 해결하기 위해 제안된 기술입니다.\\n\\nRAG는 대규모 언어 모델(LLM)에서 질문에 대한 답변이나 텍스트를 생성하기 전에 광범위한 문서 집합에서 관련 정보를 검색하고, 이를 이용하여 응답을 생성하는 방법입니다. 이는 LLM의 기존 문제점인 지식의 시대에 뒤떨어짐, 특정 영역에 대한 지식 부족, 그리고 응답의 투명성 부족을 해결하는 데 중점을 두고 있습니다.\\n\\n이러한 RAG는 답변의 정확도를 크게 향상시키고, 특히 지식 집약적인 작업에서 모델의 환각을 줄이는 데 도움이 됩니다. 사용자가 출처를 인용하여 답변의 정확성을 검증할 수 있으며, 이는 모델 출력에 대한 신뢰를 증가시킵니다. 또한, 지식 업데이트와 특정 분야의 지식 도입에도 용이합니다.\\n\\nRAG 외에 파인튜닝(fine-tuning)을 사용하여 대규모 언어 모델이 특정 지식에 집중하도록 할 수 있습니다. 하지만 이러한 파인튜닝은 실시간으로 업데이트되는 정보를 반영하는데 시간이 걸리며, 모델을 새로 학습시키는데 추가적인 자원이 소모됩니다. 이에 반해, RAG는 LLM에 특정 질의에 대한 정보를 검색할 수 있는 '참고서'를 제공하는 것과 유사합니다.\\n\\n### RAG의 3가지 주요 패러다임 - 개요\\n\\nRAG 기술의 연구 패러다임은 시간에 따라 끊임없이 발전하고 있습니다. 주요 패러다임들은 기본 RAG, 고급 RAG, 모듈형 RAG의 3가지로, 초창기의 기본 RAG는 비용 효율적이고 LLM만 사용하는 것보다 성능이 뛰어났지만 많은 단점들이 있었습니다. 고급 RAG와 모듈러 RAG의 등장은 기본 RAG의 특정 결함을 해결하기 위한 것이었습니다.\\n\\n### RAG의 3가지 주요 패러다임 - 기본 RAG(Naive RAG)\\n\\n기본 RAG는 RAG 연구의 초기 방법론을 지칭하며, 전통적인 인덱싱, 검색 및 생성 과정을 포함하고 있습니다. 기본 RAG는 주로 간단한 검색 및 생성 방식에 초점을 맞추고 있으며, RAG의 기본 개념과 원리를 설명하는데 중요합니다.\\n\\n하지만, 낮은 검색 정확도, 응답 생성의 낮은 품질, 증강 과정의 어려움 등에 따라 발생하는 불필요한 반복, 부정확한 정보, 잘못된 문맥의 통합 등이 발생할 수 있습니다. 따라서 복잡하거나 도전적인 시나리오에 기본 RAG를 사용하기는 부적절할 수 있습니다.\\n\\n### RAG의 3가지 주요 패러다임 - 고급 RAG(Advanced RAG)\\n\\n고급 RAG는 기본 RAG의 부족한 점을 개선하기 위해 개발된 패러다임입니다. 이는 주로 검색 및 생성의 질을 향상시키기 위한 사전 및 사후 검색 방법을 포함합니다. 고급 RAG는 크게 검색 전 절차(Pre-Retrieval Process), 검색 후 절차(Post-Retrieval Process), RAG 파이프라인 최적화(RAG Pipeline Optimization)의 3단계로 나누어볼 수 있습니다:\\n\\n### RAG의 3가지 주요 패러다임 - 모듈형 RAG(Modular RAG)\\n\\n모듈식 RAG은 고급 RAG의 발전된 형태로, 기존의 RAG 프레임워크에서 한 단계 더 나아가 다양한 모듈과 기능을 통합하여 더 큰 다양성과 유연성을 제공합니다. 이 접근 방식은 RAG 시스템을 다양한 시나리오와 요구 사항에 맞게 조정할 수 있도록 하는 여러 새로운 모듈과 패턴을 포함하고 있습니다.\\n\\n### RAG 기술의 주요 구성 요소: 검색기(Retriever)\\n\\n검색기(Retriever)는 RAG라는 약어의 맨 앞 글자인 R을 나타내고 있을 정도로 시스템에서 중요한 정보 탐색 역할을 합니다. 검색기는 대규모 데이터 집합에서 관련 정보를 찾아내어 텍스트 생성을 위한 데이터들을 준비하는 작업을 합니다​​. 이러한 검색기는 LLM의 일반적인 지식과 현재, 맥락적으로 정확한 정보의 필요성 사이의 간격을 메우는 데 사용됩니다. 이는 실시간 데이터, 특정 분야 전문 지식 또는 사실 확인이 필요한 시나리오에서 특히 중요합니다​​.\\n\\n### RAG 기술의 주요 구성 요소: 생성기(Generator)\\n\\n앞에서 살펴본 검색기(Retriever)의 역할이 사용자의 입력으로부터 후보 문서들을 추출하는 것이었다면, 생성기(Generator)의 역할은 검색된 결과를 활용하여 사용자에게 제공할 답변을 생성하는 것입니다. 검색된 정보를 효과적으로 활용하여 정확하고 관련성 높은 답변을 생성하기 위해, 정보를 압축하고 순위를 다시 매기는 후처리 과정과 입력 데이터에 적응하는 최적화 과정을 수행하고 있습니다.\\n\\n### RAG 기술의 주요 구성 요소: 증강 방법(Augmentation Methods)\\n\\n이 챕터에서는 RAG에서의 증강 방법을 다음 3가지 측면으로 살펴보겠습니다.\\n\\n* 증강 단계(the stage of augmentation)\\n* 데이터 소스의 증강(augmentation data sources)\\n* 증강 절차(the process of augmentation)\\n\\n### RAG 평가 (RAG Evaluation)\\n\\nRAG 평가(RAG Evaluation)는 RAG가 얼마나 효과적으로 동작하는지를 평가하는 것입니다. 평가는 크게 2가지 종류로 나뉘는데, 하나는 검색기나 생성기 등의 각각의 모듈들에 대한 독립적인 평가(independent evaluation)과 입력으로부터 출력까지의 전체 과정을 평가하는 엔드-투-엔드 평가(end-to-end evaluation; 종단간 평가)가 있습니다. 각각의 평가 방법들과 평가 시의 지표(metric), 사용할 수 있는 프레임워크(framework) 등에 대해서 알아보도록 하겠습니다.\\n\\n### RAG의 미래 전망 (Future Prospects)\\n\\nRAG의 수직적 최적화와 수평적 확장, 그리고 RAG의 생태계에 대한 내용을 다루고 있습니다.\\n\\n---\\n\\n⚠️광고: [파이토치 한국 사용자 모임](https://discuss.pytorch.kr/?utm_source=geeknews)이 정리한 이 글이 유용하셨나요? [회원으로 가입](https://discuss.pytorch.kr/signup?utm_source=geeknews)하시면 주요 글들을 이메일로 보내드립니다! (기본은 Weekly지만 [Daily로 변경](https://discuss.pytorch.kr/my/preferences/emails?utm_source=geeknews)하실 수도 있습니다.)\\n\\n[▲](javascript:votec(21961, \\\"up\\\");)\\n\\n[959ma](/user?id=959ma) [2024-01-04](comment?id=21961)  [[-]](javascript:child_toggle(21961, 1);)\\n\\n좋은 자료 공유해주셔서 감사합니다\\n\\n답변달기\\n\\n[▲](javascript:votec(21962, \\\"up\\\");)\\n\\n[ninebow](/user?id=ninebow) [2024-01-04](comment?id=21962)  [[-]](javascript:child_toggle(21962, 0);)\\n\\n앗, 읽어주셔서 감사합니다!  \\n혹시나 읽으시면서 이상하거나 잘못된 부분을 발견하시면 알려주시기를 부탁드립니다. ^^;\\n\\n답변달기\"}]\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36magent\u001b[0m in [\u001b[1;33mexecute\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "## Naive RAG의 기본 개념과 구성 요소\n",
      "\n",
      "Naive RAG(Retrieval-Augmented Generation, 검색 증강 생성)는 RAG의 가장 기본적인 형태로, 이름 그대로 간단하고 직관적인 파이프라인 구조를 가집니다. LLM(거대 언어 모델)의 한계점인 최신 정보 부족이나 환각(Hallucination) 현상을 보완하기 위해, 외부 지식 베이스에서 관련 정보를 검색하여 답변 생성에 활용하는 기술의 핵심 원리를 담고 있습니다.\n",
      "\n",
      "쉽게 말해, LLM이 '뇌'라면 Naive RAG는 질문을 받았을 때 관련된 '참고서(외부 지식)'를 찾아보고, 그 내용을 바탕으로 답변을 생성하도록 하는 방식입니다.\n",
      "\n",
      "### Naive RAG의 핵심 구성 요소 및 작동 방식\n",
      "\n",
      "Naive RAG의 작동 파이프라인은 크게 **인덱싱(Indexing), 검색(Retrieval), 생성(Generation)** 세 단계로 나눌 수 있습니다.\n",
      "\n",
      "#### 1. 인덱싱 (Indexing): 참고서를 미리 정리하고 색인을 만드는 단계\n",
      "\n",
      "LLM이 참고할 외부 문서를 미리 처리하여 검색하기 좋은 형태로 데이터베이스에 저장하는 과정입니다.\n",
      "\n",
      "*   **데이터 로드 (Data Loading):** PDF, TXT, 웹페이지 등 다양한 형태의 원본 문서를 불러옵니다.\n",
      "*   **분할 (Splitting/Chunking):** 불러온 문서를 LLM이 처리하기 쉽도록 의미 있는 작은 단위(Chunk)로 나눕니다. 너무 길면 검색 효율이 떨어지고, 너무 짧으면 의미가 부족할 수 있어 적절한 크기로 나누는 것이 중요합니다.\n",
      "*   **임베딩 (Embedding):** 텍스트로 된 각 문서 조각(Chunk)을 임베딩 모델을 사용해 의미를 함축한 숫자 벡터(Vector)로 변환합니다. 이 벡터는 텍스트 간의 의미적 유사도를 계산하는 데 사용됩니다.\n",
      "*   **저장 (Storing):** 변환된 벡터들을 벡터 데이터베이스(Vector Database)에 저장합니다. 이때 원본 텍스트와 벡터를 함께 저장하여 나중에 원문을 참조할 수 있도록 합니다.\n",
      "\n",
      "#### 2. 검색 (Retrieval): 질문과 가장 관련 있는 내용을 찾는 단계\n",
      "\n",
      "사용자가 질문을 하면, 인덱싱 단계에서 구축한 벡터 데이터베이스에서 가장 관련성 높은 문서 조각을 찾아내는 과정입니다.\n",
      "\n",
      "*   **쿼리 임베딩 (Query Embedding):** 사용자의 질문(Query)을 인덱싱 때 사용했던 것과 **동일한 임베딩 모델**을 사용해 벡터로 변환합니다.\n",
      "*   **유사도 검색 (Similarity Search):** 질문 벡터와 벡터 데이터베이스에 저장된 문서 조각 벡터들 간의 유사도(주로 코사인 유사도)를 계산합니다.\n",
      "*   **문서 반환 (Return Documents):** 유사도가 가장 높은 상위 K개의 문서 조각(Top-K)을 검색 결과로 반환합니다.\n",
      "\n",
      "#### 3. 생성 (Generation): 찾은 정보로 답변을 만드는 단계\n",
      "\n",
      "검색된 문서 조각과 사용자의 질문을 함께 LLM에 전달하여 최종 답변을 생성하는 과정입니다.\n",
      "\n",
      "*   **프롬프트 구성 (Prompting):** 검색된 문서 조각(Context)과 사용자의 원본 질문(Query)을 정해진 템플릿에 따라 하나의 프롬프트로 결합합니다.\n",
      "*   **답변 생성 (Response Generation):** 완성된 프롬프트를 LLM에 입력합니다. LLM은 주어진 컨텍스트(검색된 문서 내용)를 기반으로 사용자의 질문에 대한 답변을 생성합니다. 이를 통해 사실에 기반하고, 출처가 명확한 답변을 제공할 수 있게 됩니다.\n",
      "\n",
      "이처럼 Naive RAG는 '인덱싱 → 검색 → 생성'이라는 직관적인 흐름을 통해 LLM의 성능을 강화하는 가장 기본적인 RAG 구현 방식입니다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mexecute\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "('Naive RAG의 기본 개념과 구성 요소를 설명합니다.', \"## Naive RAG의 기본 개념과 구성 요소\\n\\nNaive RAG(Retrieval-Augmented Generation, 검색 증강 생성)는 RAG의 가장 기본적인 형태로, 이름 그대로 간단하고 직관적인 파이프라인 구조를 가집니다. LLM(거대 언어 모델)의 한계점인 최신 정보 부족이나 환각(Hallucination) 현상을 보완하기 위해, 외부 지식 베이스에서 관련 정보를 검색하여 답변 생성에 활용하는 기술의 핵심 원리를 담고 있습니다.\\n\\n쉽게 말해, LLM이 '뇌'라면 Naive RAG는 질문을 받았을 때 관련된 '참고서(외부 지식)'를 찾아보고, 그 내용을 바탕으로 답변을 생성하도록 하는 방식입니다.\\n\\n### Naive RAG의 핵심 구성 요소 및 작동 방식\\n\\nNaive RAG의 작동 파이프라인은 크게 **인덱싱(Indexing), 검색(Retrieval), 생성(Generation)** 세 단계로 나눌 수 있습니다.\\n\\n#### 1. 인덱싱 (Indexing): 참고서를 미리 정리하고 색인을 만드는 단계\\n\\nLLM이 참고할 외부 문서를 미리 처리하여 검색하기 좋은 형태로 데이터베이스에 저장하는 과정입니다.\\n\\n*   **데이터 로드 (Data Loading):** PDF, TXT, 웹페이지 등 다양한 형태의 원본 문서를 불러옵니다.\\n*   **분할 (Splitting/Chunking):** 불러온 문서를 LLM이 처리하기 쉽도록 의미 있는 작은 단위(Chunk)로 나눕니다. 너무 길면 검색 효율이 떨어지고, 너무 짧으면 의미가 부족할 수 있어 적절한 크기로 나누는 것이 중요합니다.\\n*   **임베딩 (Embedding):** 텍스트로 된 각 문서 조각(Chunk)을 임베딩 모델을 사용해 의미를 함축한 숫자 벡터(Vector)로 변환합니다. 이 벡터는 텍스트 간의 의미적 유사도를 계산하는 데 사용됩니다.\\n*   **저장 (Storing):** 변환된 벡터들을 벡터 데이터베이스(Vector Database)에 저장합니다. 이때 원본 텍스트와 벡터를 함께 저장하여 나중에 원문을 참조할 수 있도록 합니다.\\n\\n#### 2. 검색 (Retrieval): 질문과 가장 관련 있는 내용을 찾는 단계\\n\\n사용자가 질문을 하면, 인덱싱 단계에서 구축한 벡터 데이터베이스에서 가장 관련성 높은 문서 조각을 찾아내는 과정입니다.\\n\\n*   **쿼리 임베딩 (Query Embedding):** 사용자의 질문(Query)을 인덱싱 때 사용했던 것과 **동일한 임베딩 모델**을 사용해 벡터로 변환합니다.\\n*   **유사도 검색 (Similarity Search):** 질문 벡터와 벡터 데이터베이스에 저장된 문서 조각 벡터들 간의 유사도(주로 코사인 유사도)를 계산합니다.\\n*   **문서 반환 (Return Documents):** 유사도가 가장 높은 상위 K개의 문서 조각(Top-K)을 검색 결과로 반환합니다.\\n\\n#### 3. 생성 (Generation): 찾은 정보로 답변을 만드는 단계\\n\\n검색된 문서 조각과 사용자의 질문을 함께 LLM에 전달하여 최종 답변을 생성하는 과정입니다.\\n\\n*   **프롬프트 구성 (Prompting):** 검색된 문서 조각(Context)과 사용자의 원본 질문(Query)을 정해진 템플릿에 따라 하나의 프롬프트로 결합합니다.\\n*   **답변 생성 (Response Generation):** 완성된 프롬프트를 LLM에 입력합니다. LLM은 주어진 컨텍스트(검색된 문서 내용)를 기반으로 사용자의 질문에 대한 답변을 생성합니다. 이를 통해 사실에 기반하고, 출처가 명확한 답변을 제공할 수 있게 됩니다.\\n\\n이처럼 Naive RAG는 '인덱싱 → 검색 → 생성'이라는 직관적인 흐름을 통해 LLM의 성능을 강화하는 가장 기본적인 RAG 구현 방식입니다.\")\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'The model `gemini-2.5-pro` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      4\u001b[39m config = RunnableConfig(recursion_limit=\u001b[32m50\u001b[39m, configurable={\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: random_uuid()})\n\u001b[32m      6\u001b[39m inputs = {\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mModular RAG 가 기존의 Naive RAG 와 어떤 차이가 있는지와 production level 에서 사용하는 이점을 설명해줘\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43minvoke_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SBA\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-Us6BDj1P-py3.11\\Lib\\site-packages\\langchain_teddynote\\messages.py:409\u001b[39m, in \u001b[36minvoke_graph\u001b[39m\u001b[34m(graph, inputs, config, node_names, callback)\u001b[39m\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m namespace[-\u001b[32m1\u001b[39m].split(\u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(namespace) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mroot graph\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;66;03m# subgraphs=True 를 통해 서브그래프의 출력도 포함\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    411\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# node_names가 비어있지 않은 경우에만 필터링\u001b[39;49;00m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnode_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode_names\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SBA\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-Us6BDj1P-py3.11\\Lib\\site-packages\\langgraph\\pregel\\main.py:2647\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2645\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2646\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2647\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2654\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2657\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SBA\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-Us6BDj1P-py3.11\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:253\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m.\u001b[49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpanic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tb := exc.__traceback__:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SBA\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-Us6BDj1P-py3.11\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:511\u001b[39m, in \u001b[36m_panic_or_proceed\u001b[39m\u001b[34m(futs, timeout_exc_cls, panic)\u001b[39m\n\u001b[32m    509\u001b[39m                 interrupts.append(exc)\n\u001b[32m    510\u001b[39m             \u001b[38;5;28;01melif\u001b[39;00m fut \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SKIP_RERAISE_SET:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    512\u001b[39m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SBA\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-Us6BDj1P-py3.11\\Lib\\site-packages\\langgraph\\pregel\\_executor.py:81\u001b[39m, in \u001b[36mBackgroundExecutor.done\u001b[39m\u001b[34m(self, task)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mself\u001b[39m.tasks.pop(task)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\concurrent\\futures\\thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SBA\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-Us6BDj1P-py3.11\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SBA\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-Us6BDj1P-py3.11\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:657\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SBA\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-Us6BDj1P-py3.11\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:401\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mreplan_step\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreplan_step\u001b[39m(state: PlanExecute):\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     output = \u001b[43mreplanner\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# 응답이 사용자에게 반환될 경우\u001b[39;00m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output.action, Response):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SBA\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-Us6BDj1P-py3.11\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3082\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3080\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3081\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3082\u001b[39m                 input_ = context.run(step.invoke, input_, config)\n\u001b[32m   3083\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3084\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SBA\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-Us6BDj1P-py3.11\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5495\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5488\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5489\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5490\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5493\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5494\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5495\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5496\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5497\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5498\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SBA\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-Us6BDj1P-py3.11\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:393\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    383\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    388\u001b[39m     **kwargs: Any,\n\u001b[32m    389\u001b[39m ) -> BaseMessage:\n\u001b[32m    390\u001b[39m     config = ensure_config(config)\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    392\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    403\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SBA\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-Us6BDj1P-py3.11\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1019\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1010\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1012\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1016\u001b[39m     **kwargs: Any,\n\u001b[32m   1017\u001b[39m ) -> LLMResult:\n\u001b[32m   1018\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1019\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SBA\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-Us6BDj1P-py3.11\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:837\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    834\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    835\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    836\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    843\u001b[39m         )\n\u001b[32m    844\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    845\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SBA\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-Us6BDj1P-py3.11\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1085\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1083\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1084\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1085\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1089\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SBA\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-Us6BDj1P-py3.11\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1150\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1148\u001b[39m payload.pop(\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1150\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1152\u001b[39m     _handle_openai_bad_request(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SBA\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-Us6BDj1P-py3.11\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:183\u001b[39m, in \u001b[36mCompletions.parse\u001b[39m\u001b[34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, safety_identifier, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparser\u001b[39m(raw_completion: ChatCompletion) -> ParsedChatCompletion[ResponseFormatT]:\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[32m    178\u001b[39m         response_format=response_format,\n\u001b[32m    179\u001b[39m         chat_completion=raw_completion,\n\u001b[32m    180\u001b[39m         input_tools=chat_completion_tools,\n\u001b[32m    181\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;49;00m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedChatCompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SBA\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-Us6BDj1P-py3.11\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SBA\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-kr-Us6BDj1P-py3.11\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'error': {'message': 'The model `gemini-2.5-pro` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
      "During task with name 'replan' and id '71272916-e1f6-1dfe-f094-1bba285bac7c'"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import invoke_graph, random_uuid\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(recursion_limit=50, configurable={\"thread_id\": random_uuid()})\n",
    "\n",
    "inputs = {\n",
    "    \"input\": \"Modular RAG 가 기존의 Naive RAG 와 어떤 차이가 있는지와 production level 에서 사용하는 이점을 설명해줘\"\n",
    "}\n",
    "\n",
    "invoke_graph(app, inputs, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e139d290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Modular RAG와 Naive RAG의 차이점 및 Production Level에서의 이점\n",
      "\n",
      "## 서론\n",
      "\n",
      "RAG(Retrieval-Augmented Generation)는 정보 검색과 생성 모델을 결합하여 질문에 대한 답변을 생성하는 시스템입니다. 이 보고서에서는 Naive RAG와 Modular RAG의 차이점을 설명하고, Modular RAG가 production level에서 제공하는 이점을 구체적으로 살펴보겠습니다.\n",
      "\n",
      "## Naive RAG의 개념 및 구성 요소\n",
      "\n",
      "Naive RAG는 기본적으로 두 가지 주요 단계로 구성됩니다: 정보 검색과 답변 생성. 사용자가 질문을 입력하면, 시스템은 관련 문서를 검색하고, 검색된 문서를 기반으로 답변을 생성합니다. 이 방식은 구조가 단순하여 구현이 용이하지만, 복잡한 쿼리 처리나 다양한 데이터 소스의 통합에는 한계가 있을 수 있습니다.\n",
      "\n",
      "### Naive RAG의 구성 요소\n",
      "\n",
      "1. **질문 처리**: 사용자가 입력한 질문을 처리하여 검색에 적합한 형태로 변환합니다.\n",
      "2. **정보 검색**: 질문과 관련된 문서를 검색합니다.\n",
      "3. **문서 선택**: 검색된 문서 중에서 가장 관련성이 높은 문서를 선택합니다.\n",
      "4. **답변 생성**: 선택된 문서를 기반으로 생성 모델이 답변을 생성합니다.\n",
      "5. **결과 출력**: 생성된 답변을 사용자에게 제공합니다.\n",
      "\n",
      "## Modular RAG의 개념 및 구성 요소\n",
      "\n",
      "Modular RAG는 Naive RAG의 기본 구조에 여러 모듈을 추가하여 확장성을 높인 방식입니다. 주요 모듈로는 쿼리 변환, 라우팅, 재정렬 등이 있습니다. 이러한 모듈을 통해 더 복잡한 쿼리 처리와 다양한 데이터 소스의 통합을 가능하게 하며, 이는 성능과 확장성, 유지보수성 측면에서 큰 이점을 제공합니다.\n",
      "\n",
      "### Modular RAG의 주요 모듈\n",
      "\n",
      "1. **쿼리 변환 모듈**: 사용자의 질문을 보다 효과적으로 처리하기 위해 쿼리를 변환하거나 확장합니다.\n",
      "2. **라우팅 모듈**: 다양한 데이터 소스나 검색 엔진을 활용할 수 있도록 쿼리를 적절한 경로로 라우팅합니다.\n",
      "3. **재정렬 모듈**: 검색된 문서나 생성된 답변을 사용자의 의도에 맞게 재정렬하여 최적의 결과를 제공합니다.\n",
      "\n",
      "## Production Level에서 Modular RAG의 이점\n",
      "\n",
      "### 성능 향상\n",
      "\n",
      "- **정확한 정보 제공**: 최신 정보를 검색하고 이를 생성 모델에 제공하여 더 정확하고 신뢰할 수 있는 응답을 생성합니다.\n",
      "- **효율적인 처리**: 검색 모듈이 필요한 정보를 사전에 필터링하여 생성 모델이 처리해야 할 데이터의 양을 줄임으로써, 처리 속도를 향상시킵니다.\n",
      "\n",
      "### 확장성\n",
      "\n",
      "- **모듈 교체 용이**: 각 모듈이 독립적으로 설계되어 있어, 특정 모듈을 업그레이드하거나 교체할 때 전체 시스템에 미치는 영향을 최소화할 수 있습니다.\n",
      "- **다양한 데이터 소스 통합**: 여러 데이터 소스를 쉽게 통합할 수 있어, 다양한 유형의 데이터를 활용한 응답 생성이 가능합니다.\n",
      "\n",
      "### 유지보수성\n",
      "\n",
      "- **독립적 모듈 관리**: 각 모듈이 독립적으로 관리되므로, 특정 모듈에 문제가 발생했을 때 해당 모듈만 수정하거나 교체하면 됩니다.\n",
      "- **버전 관리 용이**: 모듈별로 버전을 관리할 수 있어, 특정 기능이나 성능 개선을 위한 테스트 및 배포가 용이합니다.\n",
      "\n",
      "## 결론\n",
      "\n",
      "모듈형 RAG는 전통적인 RAG에 비해 더 큰 유연성과 확장성을 제공하여 다양한 응용 분야에 적합한 솔루션을 제공합니다. 특히 대규모 시스템에서 Modular RAG를 채택할 때, 시스템의 유연성과 효율성을 극대화하는 데 기여합니다. 이러한 이점들은 복잡한 쿼리 처리와 다양한 데이터 소스의 통합을 가능하게 하여, production level에서의 성능을 크게 향상시킵니다.\n"
     ]
    }
   ],
   "source": [
    "snapshot = app.get_state(config).values\n",
    "print(snapshot[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "611af101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Modular RAG와 Naive RAG의 차이점 및 Production Level에서의 이점\n",
       "\n",
       "## 서론\n",
       "\n",
       "RAG(Retrieval-Augmented Generation)는 정보 검색과 생성 모델을 결합하여 질문에 대한 답변을 생성하는 시스템입니다. 이 보고서에서는 Naive RAG와 Modular RAG의 차이점을 설명하고, Modular RAG가 production level에서 제공하는 이점을 구체적으로 살펴보겠습니다.\n",
       "\n",
       "## Naive RAG의 개념 및 구성 요소\n",
       "\n",
       "Naive RAG는 기본적으로 두 가지 주요 단계로 구성됩니다: 정보 검색과 답변 생성. 사용자가 질문을 입력하면, 시스템은 관련 문서를 검색하고, 검색된 문서를 기반으로 답변을 생성합니다. 이 방식은 구조가 단순하여 구현이 용이하지만, 복잡한 쿼리 처리나 다양한 데이터 소스의 통합에는 한계가 있을 수 있습니다.\n",
       "\n",
       "### Naive RAG의 구성 요소\n",
       "\n",
       "1. **질문 처리**: 사용자가 입력한 질문을 처리하여 검색에 적합한 형태로 변환합니다.\n",
       "2. **정보 검색**: 질문과 관련된 문서를 검색합니다.\n",
       "3. **문서 선택**: 검색된 문서 중에서 가장 관련성이 높은 문서를 선택합니다.\n",
       "4. **답변 생성**: 선택된 문서를 기반으로 생성 모델이 답변을 생성합니다.\n",
       "5. **결과 출력**: 생성된 답변을 사용자에게 제공합니다.\n",
       "\n",
       "## Modular RAG의 개념 및 구성 요소\n",
       "\n",
       "Modular RAG는 Naive RAG의 기본 구조에 여러 모듈을 추가하여 확장성을 높인 방식입니다. 주요 모듈로는 쿼리 변환, 라우팅, 재정렬 등이 있습니다. 이러한 모듈을 통해 더 복잡한 쿼리 처리와 다양한 데이터 소스의 통합을 가능하게 하며, 이는 성능과 확장성, 유지보수성 측면에서 큰 이점을 제공합니다.\n",
       "\n",
       "### Modular RAG의 주요 모듈\n",
       "\n",
       "1. **쿼리 변환 모듈**: 사용자의 질문을 보다 효과적으로 처리하기 위해 쿼리를 변환하거나 확장합니다.\n",
       "2. **라우팅 모듈**: 다양한 데이터 소스나 검색 엔진을 활용할 수 있도록 쿼리를 적절한 경로로 라우팅합니다.\n",
       "3. **재정렬 모듈**: 검색된 문서나 생성된 답변을 사용자의 의도에 맞게 재정렬하여 최적의 결과를 제공합니다.\n",
       "\n",
       "## Production Level에서 Modular RAG의 이점\n",
       "\n",
       "### 성능 향상\n",
       "\n",
       "- **정확한 정보 제공**: 최신 정보를 검색하고 이를 생성 모델에 제공하여 더 정확하고 신뢰할 수 있는 응답을 생성합니다.\n",
       "- **효율적인 처리**: 검색 모듈이 필요한 정보를 사전에 필터링하여 생성 모델이 처리해야 할 데이터의 양을 줄임으로써, 처리 속도를 향상시킵니다.\n",
       "\n",
       "### 확장성\n",
       "\n",
       "- **모듈 교체 용이**: 각 모듈이 독립적으로 설계되어 있어, 특정 모듈을 업그레이드하거나 교체할 때 전체 시스템에 미치는 영향을 최소화할 수 있습니다.\n",
       "- **다양한 데이터 소스 통합**: 여러 데이터 소스를 쉽게 통합할 수 있어, 다양한 유형의 데이터를 활용한 응답 생성이 가능합니다.\n",
       "\n",
       "### 유지보수성\n",
       "\n",
       "- **독립적 모듈 관리**: 각 모듈이 독립적으로 관리되므로, 특정 모듈에 문제가 발생했을 때 해당 모듈만 수정하거나 교체하면 됩니다.\n",
       "- **버전 관리 용이**: 모듈별로 버전을 관리할 수 있어, 특정 기능이나 성능 개선을 위한 테스트 및 배포가 용이합니다.\n",
       "\n",
       "## 결론\n",
       "\n",
       "모듈형 RAG는 전통적인 RAG에 비해 더 큰 유연성과 확장성을 제공하여 다양한 응용 분야에 적합한 솔루션을 제공합니다. 특히 대규모 시스템에서 Modular RAG를 채택할 때, 시스템의 유연성과 효율성을 극대화하는 데 기여합니다. 이러한 이점들은 복잡한 쿼리 처리와 다양한 데이터 소스의 통합을 가능하게 하여, production level에서의 성능을 크게 향상시킵니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(snapshot[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fde7887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Modular RAG와 Naive RAG의 차이점 및 Production Level에서의 이점\n",
      "\n",
      "## 서론\n",
      "\n",
      "RAG(Retrieval-Augmented Generation)는 정보 검색과 생성 모델을 결합하여 질문에 대한 답변을 생성하는 시스템입니다. 이 보고서에서는 Naive RAG와 Modular RAG의 차이점을 설명하고, Modular RAG가 production level에서 제공하는 이점을 구체적으로 살펴보겠습니다.\n",
      "\n",
      "## Naive RAG의 개념 및 구성 요소\n",
      "\n",
      "Naive RAG는 기본적으로 두 가지 주요 단계로 구성됩니다: 정보 검색과 답변 생성. 사용자가 질문을 입력하면, 시스템은 관련 문서를 검색하고, 검색된 문서를 기반으로 답변을 생성합니다. 이 방식은 구조가 단순하여 구현이 용이하지만, 복잡한 쿼리 처리나 다양한 데이터 소스의 통합에는 한계가 있을 수 있습니다.\n",
      "\n",
      "### Naive RAG의 구성 요소\n",
      "\n",
      "1. **질문 처리**: 사용자가 입력한 질문을 처리하여 검색에 적합한 형태로 변환합니다.\n",
      "2. **정보 검색**: 질문과 관련된 문서를 검색합니다.\n",
      "3. **문서 선택**: 검색된 문서 중에서 가장 관련성이 높은 문서를 선택합니다.\n",
      "4. **답변 생성**: 선택된 문서를 기반으로 생성 모델이 답변을 생성합니다.\n",
      "5. **결과 출력**: 생성된 답변을 사용자에게 제공합니다.\n",
      "\n",
      "## Modular RAG의 개념 및 구성 요소\n",
      "\n",
      "Modular RAG는 Naive RAG의 기본 구조에 여러 모듈을 추가하여 확장성을 높인 방식입니다. 주요 모듈로는 쿼리 변환, 라우팅, 재정렬 등이 있습니다. 이러한 모듈을 통해 더 복잡한 쿼리 처리와 다양한 데이터 소스의 통합을 가능하게 하며, 이는 성능과 확장성, 유지보수성 측면에서 큰 이점을 제공합니다.\n",
      "\n",
      "### Modular RAG의 주요 모듈\n",
      "\n",
      "1. **쿼리 변환 모듈**: 사용자의 질문을 보다 효과적으로 처리하기 위해 쿼리를 변환하거나 확장합니다.\n",
      "2. **라우팅 모듈**: 다양한 데이터 소스나 검색 엔진을 활용할 수 있도록 쿼리를 적절한 경로로 라우팅합니다.\n",
      "3. **재정렬 모듈**: 검색된 문서나 생성된 답변을 사용자의 의도에 맞게 재정렬하여 최적의 결과를 제공합니다.\n",
      "\n",
      "## Production Level에서 Modular RAG의 이점\n",
      "\n",
      "### 성능 향상\n",
      "\n",
      "- **정확한 정보 제공**: 최신 정보를 검색하고 이를 생성 모델에 제공하여 더 정확하고 신뢰할 수 있는 응답을 생성합니다.\n",
      "- **효율적인 처리**: 검색 모듈이 필요한 정보를 사전에 필터링하여 생성 모델이 처리해야 할 데이터의 양을 줄임으로써, 처리 속도를 향상시킵니다.\n",
      "\n",
      "### 확장성\n",
      "\n",
      "- **모듈 교체 용이**: 각 모듈이 독립적으로 설계되어 있어, 특정 모듈을 업그레이드하거나 교체할 때 전체 시스템에 미치는 영향을 최소화할 수 있습니다.\n",
      "- **다양한 데이터 소스 통합**: 여러 데이터 소스를 쉽게 통합할 수 있어, 다양한 유형의 데이터를 활용한 응답 생성이 가능합니다.\n",
      "\n",
      "### 유지보수성\n",
      "\n",
      "- **독립적 모듈 관리**: 각 모듈이 독립적으로 관리되므로, 특정 모듈에 문제가 발생했을 때 해당 모듈만 수정하거나 교체하면 됩니다.\n",
      "- **버전 관리 용이**: 모듈별로 버전을 관리할 수 있어, 특정 기능이나 성능 개선을 위한 테스트 및 배포가 용이합니다.\n",
      "\n",
      "## 결론\n",
      "\n",
      "모듈형 RAG는 전통적인 RAG에 비해 더 큰 유연성과 확장성을 제공하여 다양한 응용 분야에 적합한 솔루션을 제공합니다. 특히 대규모 시스템에서 Modular RAG를 채택할 때, 시스템의 유연성과 효율성을 극대화하는 데 기여합니다. 이러한 이점들은 복잡한 쿼리 처리와 다양한 데이터 소스의 통합을 가능하게 하여, production level에서의 성능을 크게 향상시킵니다.\n"
     ]
    }
   ],
   "source": [
    "print(snapshot[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c1cf27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebbda33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-Us6BDj1P-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
